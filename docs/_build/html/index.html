

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>FlexiTransformers Documentation &mdash; FlexiTransformers 0.1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />

  
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=01f34227"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="_static/copybutton.js?v=f281be69"></script>
      <script src="_static/design-tabs.js?v=f930bc37"></script>
      <script type="module" src="https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs"></script>
      <script type="module" src="https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs"></script>
      <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";import elkLayouts from "https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs";mermaid.registerLayoutLoaders(elkLayouts);mermaid.initialize({startOnLoad:false});</script>
      <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
      <script type="module">
import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";
window.addEventListener("load", () => mermaid.run());
</script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="flexit" href="modules.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="#" class="icon icon-home">
            FlexiTransformers
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Package Modules</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="modules.html">flexit</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">FlexiTransformers</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="#" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">FlexiTransformers Documentation</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/index.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="flexitransformers-documentation">
<h1>FlexiTransformers Documentation<a class="headerlink" href="#flexitransformers-documentation" title="Link to this heading"></a></h1>
<img alt="FlexiTransformers Logo" class="align-center" src="_images/logo.png" />
<a class="reference external image-reference" href="https://opensource.org/licenses/MIT"><img alt="License" src="https://img.shields.io/badge/License-MIT-yellow.svg" />
</a>
<a class="reference external image-reference" href="https://pypi.org/project/flexitransformers/"><img alt="PyPI version" src="https://badge.fury.io/py/flexitransformers.svg" />
</a>
<a class="reference external image-reference" href="https://flexitransformers.readthedocs.io/en/latest/?badge=latest"><img alt="Documentation Status" src="https://readthedocs.org/projects/flexitransformers/badge/?version=latest" />
</a>
<a class="reference external image-reference" href="https://github.com/astral-sh/ruff"><img alt="Code Style: Ruff" src="https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json" />
</a>
<p>A modular Transformer framework supporting multiple architectures and attention mechanisms.</p>
<pre  class="mermaid">
        graph TD
    A[FlexiTransformers] --&gt; B[Architectures]
    A --&gt; C[Components]
    A --&gt; D[Training]

    B --&gt; B1[Encoder-Decoder]
    B --&gt; B2[Encoder-Only]
    B --&gt; B3[Decoder-Only]

    C --&gt; C1[Attention Mechanisms]
    C --&gt; C2[Positional Embeddings]
    C --&gt; C3[Layer Implementations]

    C1 --&gt; C1a[Absolute]
    C1 --&gt; C1b[Rotary/RoPE]
    C1 --&gt; C1c[ALiBi]
    C1 --&gt; C1d[Relative Global]

    D --&gt; D1[Trainer Class]
    D --&gt; D2[Callbacks]
    D --&gt; D3[Loss Functions]
    </pre><section id="features">
<h2>Features<a class="headerlink" href="#features" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p><strong>Multi-Architecture Support</strong>: Build Encoder-Decoder, Encoder-only (BERT), or Decoder-only (GPT) models</p></li>
<li><p><strong>Modular Components</strong>: Swap attention mechanisms, positional embeddings, and feed-forward layers</p></li>
<li><p><strong>Production-Ready</strong>: Type hints, comprehensive docs, and CI/CD tested</p></li>
<li><p><strong>Training Utilities</strong>: Built-in trainer with progress monitoring and callback system</p></li>
<li><p><strong>10+ Attention Variants</strong>: Includes Absolute, Rotary (RoPE), ALiBi, and Relative Global Attention</p></li>
</ul>
</section>
<section id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Link to this heading"></a></h2>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>flexitransformers
</pre></div>
</div>
</section>
<section id="quick-start">
<h2>Quick Start<a class="headerlink" href="#quick-start" title="Link to this heading"></a></h2>
<section id="encoder-decoder-model">
<h3>Encoder-Decoder Model<a class="headerlink" href="#encoder-decoder-model" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">flexit.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">TransformerModel</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">TransformerModel</span><span class="p">(</span>
 <span class="s1">&#39;src_vocab&#39;</span><span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>
 <span class="s1">&#39;tgt_vocab&#39;</span><span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>
 <span class="s1">&#39;d_model&#39;</span><span class="p">:</span> <span class="mi">768</span><span class="p">,</span>
 <span class="s1">&#39;n_heads&#39;</span><span class="p">:</span> <span class="mi">12</span><span class="p">,</span>
 <span class="s1">&#39;n_layers&#39;</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span>
 <span class="s1">&#39;pe_type&#39;</span><span class="p">:</span> <span class="s1">&#39;absolute&#39;</span><span class="p">,</span>
 <span class="s1">&#39;init_method&#39;</span><span class="p">:</span> <span class="s1">&#39;xavier_uniform&#39;</span><span class="p">,</span>
 <span class="s1">&#39;pre_norm&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="bert-style-model">
<h3>BERT-Style Model<a class="headerlink" href="#bert-style-model" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">flexit.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">FlexiBERT</span>

<span class="n">bert_model</span> <span class="o">=</span> <span class="n">FlexiBERT</span><span class="p">(</span>
    <span class="n">src_vocab</span><span class="o">=</span><span class="mi">32000</span><span class="p">,</span>
    <span class="n">num_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">d_model</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span>
    <span class="n">pe_type</span><span class="o">=</span><span class="s1">&#39;alibi&#39;</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="gpt-style-model">
<h3>GPT-Style Model<a class="headerlink" href="#gpt-style-model" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">flexit.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">FlexiGPT</span>

<span class="n">gpt_model</span> <span class="o">=</span> <span class="n">FlexiGPT</span><span class="p">(</span>
    <span class="n">tgt_vocab</span><span class="o">=</span><span class="mi">50000</span><span class="p">,</span>
    <span class="n">d_model</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
    <span class="n">pe_type</span><span class="o">=</span><span class="s1">&#39;rotary&#39;</span>
<span class="p">)</span>
</pre></div>
</div>
<pre  class="mermaid">
        flowchart LR
 A[Configuration] --&gt; B[TransformerFactory]
 B --&gt; C{Architecture}
 C --&gt;|Encoder-Decoder| D[EncoderDecoderArchitectures]
 C --&gt;|Encoder-Only| E[EncoderOnlyArchitectures]
 C --&gt;|Decoder-Only| F[DecoderOnlArchitectures]
 D --&gt;|Dedicated Class| G[FlexiTransformer]
 D --&gt;|Generic Class| H[TransformerModel]
 E --&gt;|Dedicated Class| I[FlexiBERT]
 E --&gt;|Generic Class| J[FlexiTransformer]
 F --&gt;|Dedicated Class| K[FlexiGPT]
 F --&gt;|Generic Class| L[FlexiTransformer]
 G --&gt; M[Model Instance]
 H --&gt; M
 I --&gt; M
 J --&gt; M
 K --&gt; M
 L --&gt; M
    </pre></section>
</section>
<section id="core-architecture">
<h2>Core Architecture<a class="headerlink" href="#core-architecture" title="Link to this heading"></a></h2>
<p>Three Fundamental Components:</p>
<ol class="arabic simple">
<li><p><strong>Embedding System</strong>
- Token embeddings + Positional Encoding
- Supports Absolute, Rotary (RoPE), and ALiBi encoding</p></li>
<li><p><strong>Transformer Layers</strong>
- Configurable encoder/decoder stacks
- Layer normalization variants (pre-norm/post-norm)</p></li>
<li><p><strong>Task Heads</strong>
- Sequence generation (Encoder-Decoder)
- Classification (Encoder-only)
- Autoregressive generation (Decoder-only)</p></li>
</ol>
<pre  class="mermaid">
        stateDiagram-v2
    [*] --&gt; InputEmbedding
    InputEmbedding --&gt; PositionalEncoding
    PositionalEncoding --&gt; TransformerLayers

    state TransformerLayers {
        [*] --&gt; EncoderStack
        EncoderStack --&gt; [*]: Encoder-Only
        EncoderStack --&gt; DecoderStack: Encoder-Decoder
        [*] --&gt; DecoderStack: Decoder-Only
        DecoderStack --&gt; [*]
    }

    TransformerLayers --&gt; TaskHead
    TaskHead --&gt; [*]
    </pre></section>
<section id="training-pipeline">
<h2>Training Pipeline<a class="headerlink" href="#training-pipeline" title="Link to this heading"></a></h2>
<p>Example Training Loop:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">flexit.train</span><span class="w"> </span><span class="kn">import</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">Batch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">flexit.loss</span><span class="w"> </span><span class="kn">import</span> <span class="n">LossCompute</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">flexit.callbacks</span><span class="w"> </span><span class="kn">import</span> <span class="n">CheckpointCallback</span><span class="p">,</span> <span class="n">EarlyStoppingCallback</span>

<span class="c1"># Initialize components</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">),</span>
    <span class="n">loss_fn</span><span class="o">=</span><span class="n">LossCompute</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">generator</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()),</span>
    <span class="n">train_dataloader</span><span class="o">=</span><span class="n">train_loader</span><span class="p">,</span>
    <span class="n">val_dataloader</span><span class="o">=</span><span class="n">val_loader</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">CheckpointCallback</span><span class="p">(),</span> <span class="n">EarlyStoppingCallback</span><span class="p">()]</span>
<span class="p">)</span>

<span class="c1"># Run training</span>
<span class="n">metrics</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="customization-guide">
<h2>Customization Guide<a class="headerlink" href="#customization-guide" title="Link to this heading"></a></h2>
<ol class="arabic">
<li><p><strong>Configuration-Driven Models</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">flexit.configs</span><span class="w"> </span><span class="kn">import</span> <span class="n">ModelConfig</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">ModelConfig</span><span class="p">(</span>
    <span class="n">d_model</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span>
    <span class="n">n_heads</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
    <span class="n">pe_type</span><span class="o">=</span><span class="s1">&#39;relative&#39;</span><span class="p">,</span>
    <span class="n">ff_activation</span><span class="o">=</span><span class="s1">&#39;gelu&#39;</span>
<span class="p">)</span>
</pre></div>
</div>
</li>
<li><p><strong>Custom Attention Layer</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">flexit.attention</span><span class="w"> </span><span class="kn">import</span> <span class="n">AbstractAttention</span>

<span class="k">class</span><span class="w"> </span><span class="nc">LocalWindowAttention</span><span class="p">(</span><span class="n">AbstractAttention</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="c1"># Implement windowed attention</span>
        <span class="k">return</span> <span class="n">attention_output</span>
</pre></div>
</div>
</li>
<li><p><strong>Custom Positional Encoding</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">flexit.pos_embeddings</span><span class="w"> </span><span class="kn">import</span> <span class="n">AbsolutePositionalEncoding</span>

<span class="k">class</span><span class="w"> </span><span class="nc">LearnedPositionalEncoding</span><span class="p">(</span><span class="n">AbsolutePositionalEncoding</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="mi">5000</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">max_len</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pe</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">))</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="modules-overview">
<h2>Modules Overview<a class="headerlink" href="#modules-overview" title="Link to this heading"></a></h2>
<table class="docutils align-default">
<colgroup>
<col style="width: 30.0%" />
<col style="width: 70.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Module</p></th>
<th class="head"><p>Key Components</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>attention.py</p></td>
<td><p>Multi-head attention implementations</p></td>
</tr>
<tr class="row-odd"><td><p>configs.py</p></td>
<td><p>Model configuration dataclasses</p></td>
</tr>
<tr class="row-even"><td><p>factory.py</p></td>
<td><p>Model creation from configurations</p></td>
</tr>
<tr class="row-odd"><td><p>pos_embeddings.py</p></td>
<td><p>Positional encoding implementations</p></td>
</tr>
<tr class="row-even"><td><p>train.py</p></td>
<td><p>Trainer class with metrics tracking</p></td>
</tr>
<tr class="row-odd"><td><p>callbacks.py</p></td>
<td><p>Checkpointing, early stopping, custom callbacks</p></td>
</tr>
</tbody>
</table>
<pre  class="mermaid">
        classDiagram
    TransformerFactory --&gt; FlexiTransformer
    TransformerFactory --&gt; TransformerModel
    TransformerFactory --&gt; FlexiBERT
    TransformerFactory --&gt; FlexiGPT

    class EncoderDecoder {
        +Encoder encoder
        +Decoder decoder
        +Generator generator
        +forward(src, tgt, src_mask, tgt_mask)
    }

    class Attention {
        &lt;&lt;interface&gt;&gt;
        +forward(query, key, value, mask)
    }

    class PositionalEncoding {
        &lt;&lt;interface&gt;&gt;
        +forward(x)
    }

    Attention &lt;|-- AbsoluteMultiHeadedAttention
    Attention &lt;|-- RotaryMultiHeadAttention
    Attention &lt;|-- ALiBiMultiHeadAttention
    Attention &lt;|-- RelativeGlobalAttention

    PositionalEncoding &lt;|-- AbsolutePositionalEncoding
    PositionalEncoding &lt;|-- RotaryPositionalEncoding
    PositionalEncoding &lt;|-- ALiBiPositionalEncoding
    </pre></section>
<section id="api-reference">
<h2>API Reference<a class="headerlink" href="#api-reference" title="Link to this heading"></a></h2>
<div class="toctree-wrapper compound">
<p class="caption" role="heading"><span class="caption-text">Package Modules</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="modules.html">flexit</a><ul>
<li class="toctree-l2"><a class="reference internal" href="flexit.html">flexit package</a></li>
</ul>
</li>
</ul>
</div>
</section>
<section id="contributing">
<h2>Contributing<a class="headerlink" href="#contributing" title="Link to this heading"></a></h2>
<ol class="arabic simple">
<li><p>Fork the repository</p></li>
<li><p>Create feature branch</p></li>
<li><p>Submit PR with tests and documentation</p></li>
<li><p>Follow PEP8 and type hinting guidelines</p></li>
</ol>
</section>
<section id="license">
<h2>License<a class="headerlink" href="#license" title="Link to this heading"></a></h2>
<p>MIT License - See full <a class="reference external" href="https://github.com/A-Elshahawy/flexitransformers/blob/main/LICENSE">license text</a></p>
</section>
<section id="acknowledgments">
<h2>Acknowledgments<a class="headerlink" href="#acknowledgments" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p>Original Transformer paper “Attention Is All You Need”</p></li>
<li><p>Hugging Face Transformers for architectural inspiration</p></li>
<li><p>PyTorch community for foundational components</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="modules.html" class="btn btn-neutral float-right" title="flexit" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Ahmed Elshahawy.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>