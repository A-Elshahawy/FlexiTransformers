

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>flexit package &mdash; FlexiTransformers 0.1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />

  
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=01f34227"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="_static/copybutton.js?v=f281be69"></script>
      <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="flexit" href="modules.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            FlexiTransformers
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Package Modules</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="modules.html">flexit</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">flexit package</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#submodules">Submodules</a></li>
<li class="toctree-l3"><a class="reference internal" href="#flexit-attention-module">flexit.attention module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#flexit-callbacks-module">flexit.callbacks module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#flexit-configs-module">flexit.configs module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#flexit-core-module">flexit.core module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#flexit-factory-module">flexit.factory module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#flexit-layers-module">flexit.layers module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#flexit-loss-module">flexit.loss module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#flexit-models-module">flexit.models module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#flexit-models-heads-module">flexit.models_heads module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#flexit-pos-embeddings-module">flexit.pos_embeddings module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#flexit-train-module">flexit.train module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#flexit-utils-module">flexit.utils module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-contents">Module contents</a></li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">FlexiTransformers</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="modules.html">flexit</a></li>
      <li class="breadcrumb-item active">flexit package</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/flexit.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="flexit-package">
<h1>flexit package<a class="headerlink" href="#flexit-package" title="Link to this heading"></a></h1>
<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Link to this heading"></a></h2>
</section>
<section id="flexit-attention-module">
<h2>flexit.attention module<a class="headerlink" href="#flexit-attention-module" title="Link to this heading"></a></h2>
<p>Attention Mechanisms</p>
<p>This module implements various attention mechanisms used in transformer models,
including absolute, relative, rotary, and ALiBi attention implementations.</p>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">flexit.attention.</span></span><span class="sig-name descname"><span class="pre">AbstractAttention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/attention.html#AbstractAttention"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">ABC</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Abstract base class for attention mechanisms.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_heads</strong> (<em>int</em>) – Number of attention heads.</p></li>
<li><p><strong>d_model</strong> (<em>int</em>) – Model dimension.</p></li>
<li><p><strong>dropout</strong> (<em>float</em>) – Dropout probability.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">d_k</span></span></dt>
<dd><p>Dimension per head.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">n_heads</span></span></dt>
<dd><p>Number of attention heads.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">linears</span></span></dt>
<dd><p>Linear layers for query, key, value, and output projections.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>nn.ModuleList</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">attn</span></span></dt>
<dd><p>Attention weights (stored for visualization).</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">dropout</span></span></dt>
<dd><p>Dropout layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>nn.Dropout</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">attention</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/attention.html#AbstractAttention.attention"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Abstract method to compute attention scores.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/attention.html#AbstractAttention.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Forward pass for multi-headed attention.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">abstractmethod</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">attention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">query</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">key</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/attention.html#AbstractAttention.attention"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Compute attention scores and output.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>query</strong> (<em>torch.Tensor</em>) – Query tensor.</p></li>
<li><p><strong>key</strong> (<em>torch.Tensor</em>) – Key tensor.</p></li>
<li><p><strong>value</strong> (<em>torch.Tensor</em>) – Value tensor.</p></li>
<li><p><strong>mask</strong> (<em>Optional</em><em>[</em><em>torch.Tensor</em><em>]</em>) – Mask tensor. Default: None.</p></li>
<li><p><strong>dropout</strong> (<em>Optional</em><em>[</em><em>nn.Dropout</em><em>]</em>) – Dropout layer. Default: None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Output tensor and attention weights.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>tuple[torch.Tensor, torch.Tensor]</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>NotImplementedError</strong> – If not implemented by subclass.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">query</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">key</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/attention.html#AbstractAttention.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Forward pass for multi-headed attention.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>query</strong> (<em>torch.Tensor</em>) – Query tensor.</p></li>
<li><p><strong>key</strong> (<em>torch.Tensor</em>) – Key tensor.</p></li>
<li><p><strong>value</strong> (<em>torch.Tensor</em>) – Value tensor.</p></li>
<li><p><strong>mask</strong> (<em>Optional</em><em>[</em><em>torch.Tensor</em><em>]</em>) – Mask tensor. Default: None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Output tensor after applying multi-head attention.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">flexit.attention.</span></span><span class="sig-name descname"><span class="pre">AbsoluteMultiHeadedAttention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/attention.html#AbsoluteMultiHeadedAttention"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">AbstractAttention</span></code></p>
<p>Implements standard multi-headed attention with absolute positional encoding.</p>
<p>This class is based on the original Transformer paper:
“Attention is All You Need” by Vaswani et al.
Link: <a class="reference external" href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_heads</strong> (<em>int</em>) – Number of attention heads.</p></li>
<li><p><strong>d_model</strong> (<em>int</em>) – Model dimension.</p></li>
<li><p><strong>dropout</strong> (<em>float</em>) – Dropout probability.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">Inherits</span> <span class="pre">all</span> <span class="pre">attributes</span> <span class="pre">from</span> <span class="pre">AbstractAttention.</span></span></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">attention</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/attention.html#AbsoluteMultiHeadedAttention.attention"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Computes scaled dot-product attention.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/attention.html#AbsoluteMultiHeadedAttention.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Implements forward pass using parent class.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">attention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">query</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">key</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/attention.html#AbsoluteMultiHeadedAttention.attention"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Compute scaled dot-product attention.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>query</strong> (<em>torch.Tensor</em>) – Query tensor.</p></li>
<li><p><strong>key</strong> (<em>torch.Tensor</em>) – Key tensor.</p></li>
<li><p><strong>value</strong> (<em>torch.Tensor</em>) – Value tensor.</p></li>
<li><p><strong>mask</strong> (<em>Optional</em><em>[</em><em>torch.Tensor</em><em>]</em>) – Mask tensor. Default: None.</p></li>
<li><p><strong>dropout</strong> (<em>Optional</em><em>[</em><em>nn.Dropout</em><em>]</em>) – Dropout layer. Default: None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Output tensor and attention weights.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>tuple[torch.Tensor, torch.Tensor]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">query</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">key</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/attention.html#AbsoluteMultiHeadedAttention.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Forward pass for absolute multi-headed attention.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>query</strong> (<em>torch.Tensor</em>) – Query tensor.</p></li>
<li><p><strong>key</strong> (<em>torch.Tensor</em>) – Key tensor.</p></li>
<li><p><strong>value</strong> (<em>torch.Tensor</em>) – Value tensor.</p></li>
<li><p><strong>mask</strong> (<em>Optional</em><em>[</em><em>torch.Tensor</em><em>]</em>) – Mask tensor. Default: None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Output tensor after applying attention.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">flexit.attention.</span></span><span class="sig-name descname"><span class="pre">RotaryMultiHeadAttention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rope_percentage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/attention.html#RotaryMultiHeadAttention"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">AbstractAttention</span></code></p>
<p>Implements multi-headed attention with rotary positional encoding.
Based on the paper “RoFormer: Enhanced Transformer with Rotary Positional Embedding”
by Jianlin Su, et al. <a class="reference external" href="https://arxiv.org/abs/2104.09817">https://arxiv.org/abs/2104.09817</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_heads</strong> (<em>int</em>) – Number of attention heads.</p></li>
<li><p><strong>d_model</strong> (<em>int</em>) – Model dimension.</p></li>
<li><p><strong>rope_percentage</strong> (<em>float</em>) – Percentage of dimensions to apply rotary encoding to.</p></li>
<li><p><strong>dropout</strong> (<em>float</em>) – Dropout probability.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">query_rotary_pe</span></span></dt>
<dd><p>Rotary positional encoding for queries.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>RotaryPositionalEncoding</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">key_rotary_pe</span></span></dt>
<dd><p>Rotary positional encoding for keys.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>RotaryPositionalEncoding</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">attention</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/attention.html#RotaryMultiHeadAttention.attention"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Computes attention with rotary positional encoding.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/attention.html#RotaryMultiHeadAttention.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Implements forward pass using parent class.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">attention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">query</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">key</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/attention.html#RotaryMultiHeadAttention.attention"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Compute attention with rotary positional encoding.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>query</strong> (<em>torch.Tensor</em>) – Query tensor.</p></li>
<li><p><strong>key</strong> (<em>torch.Tensor</em>) – Key tensor.</p></li>
<li><p><strong>value</strong> (<em>torch.Tensor</em>) – Value tensor.</p></li>
<li><p><strong>mask</strong> (<em>Optional</em><em>[</em><em>torch.Tensor</em><em>]</em>) – Mask tensor. Default: None.</p></li>
<li><p><strong>dropout</strong> (<em>Optional</em><em>[</em><em>nn.Dropout</em><em>]</em>) – Dropout layer. Default: None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Output tensor and attention weights.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>tuple[torch.Tensor, torch.Tensor]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">query</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">key</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/attention.html#RotaryMultiHeadAttention.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Forward pass for rotary multi-headed attention.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>query</strong> (<em>torch.Tensor</em>) – Query tensor.</p></li>
<li><p><strong>key</strong> (<em>torch.Tensor</em>) – Key tensor.</p></li>
<li><p><strong>value</strong> (<em>torch.Tensor</em>) – Value tensor.</p></li>
<li><p><strong>mask</strong> (<em>Optional</em><em>[</em><em>torch.Tensor</em><em>]</em>) – Mask tensor. Default: None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Output tensor after applying attention.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">flexit.attention.</span></span><span class="sig-name descname"><span class="pre">ALiBiMultiHeadAttention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/attention.html#ALiBiMultiHeadAttention"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">AbstractAttention</span></code></p>
<p>Multi-head attention mechanism with ALiBi positional encoding.</p>
<p>ALiBi is an efficient positional encoding scheme for transformer models that
relies on linear biases for attention scores. This implementation is useful
for long-range dependencies in sequence-to-sequence tasks.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_heads</strong> (<em>int</em>) – Number of attention heads.</p></li>
<li><p><strong>d_model</strong> (<em>int</em>) – Model dimension.</p></li>
<li><p><strong>dropout</strong> (<em>float</em>) – Dropout probability.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">pe</span></span></dt>
<dd><p>ALiBi positional encoding.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>ALiBiPositionalEncoding</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">attention</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/attention.html#ALiBiMultiHeadAttention.attention"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Computes attention with ALiBi positional encoding.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/attention.html#ALiBiMultiHeadAttention.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Implements forward pass using parent class.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">attention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">query</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">key</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/attention.html#ALiBiMultiHeadAttention.attention"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Compute ALiBi multi-head attention.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>query</strong> (<em>torch.Tensor</em>) – Query tensor.</p></li>
<li><p><strong>key</strong> (<em>torch.Tensor</em>) – Key tensor.</p></li>
<li><p><strong>value</strong> (<em>torch.Tensor</em>) – Value tensor.</p></li>
<li><p><strong>mask</strong> (<em>Optional</em><em>[</em><em>torch.Tensor</em><em>]</em>) – Mask tensor. Default: None.</p></li>
<li><p><strong>dropout</strong> (<em>Optional</em><em>[</em><em>nn.Dropout</em><em>]</em>) – Dropout layer. Default: None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Output tensor and attention weights.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>tuple[torch.Tensor, torch.Tensor]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">query</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">key</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/attention.html#ALiBiMultiHeadAttention.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Forward pass for the ALiBi multi-head attention mechanism.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>query</strong> (<em>torch.Tensor</em>) – Query tensor.</p></li>
<li><p><strong>key</strong> (<em>torch.Tensor</em>) – Key tensor.</p></li>
<li><p><strong>value</strong> (<em>torch.Tensor</em>) – Value tensor.</p></li>
<li><p><strong>mask</strong> (<em>Optional</em><em>[</em><em>torch.Tensor</em><em>]</em>) – Mask tensor. Default: None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Output tensor after applying ALiBi attention.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">flexit.attention.</span></span><span class="sig-name descname"><span class="pre">RelativeGlobalAttention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1024</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chunk_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">128</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/attention.html#RelativeGlobalAttention"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">AbstractAttention</span></code></p>
<p>Implements multi-headed attention with relative positional encoding.
Reference:
- “Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context”
by Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, and
Ruslan Salakhutdinov.
(<a class="reference external" href="https://arxiv.org/abs/1906.08875">https://arxiv.org/abs/1906.08875</a>)
- “Rethinking Positional Encoding in Language Models”
by Ziheng Lin, Mingxuan Wang, and Hang Li.
(<a class="reference external" href="https://arxiv.org/abs/2006.15509">https://arxiv.org/abs/2006.15509</a>)
:type n_heads: <span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>
:param n_heads: Number of attention heads.
:type n_heads: int
:type d_model: <span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>
:param d_model: Model dimension.
:type d_model: int
:type max_len: <span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>
:param max_len: Maximum sequence length.
:type max_len: int
:type dropout: <span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></span>
:param dropout: Dropout probability.
:type dropout: float
:type chunk_size: <span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>
:param chunk_size: Chunk size for memory-efficient computation.
:type chunk_size: int</p>
<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">Er</span></span></dt>
<dd><p>Relative positional encoding parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>nn.Parameter</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">position_cache</span></span></dt>
<dd><p>Cached position indices.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">_get_relative_positions</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/attention.html#RelativeGlobalAttention._get_relative_positions"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Get relative positions for sequence.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">_chunked_attention</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/attention.html#RelativeGlobalAttention._chunked_attention"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Compute attention in chunks for memory efficiency.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">attention</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/attention.html#RelativeGlobalAttention.attention"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Computes attention with relative positional encoding.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Implements forward pass using parent class.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">attention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">query</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">key</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/attention.html#RelativeGlobalAttention.attention"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Compute attention with relative positional encoding.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>query</strong> (<em>torch.Tensor</em>) – Query tensor.</p></li>
<li><p><strong>key</strong> (<em>torch.Tensor</em>) – Key tensor.</p></li>
<li><p><strong>value</strong> (<em>torch.Tensor</em>) – Value tensor.</p></li>
<li><p><strong>mask</strong> (<em>Optional</em><em>[</em><em>torch.Tensor</em><em>]</em>) – Mask tensor. Default: None.</p></li>
<li><p><strong>dropout</strong> (<em>Optional</em><em>[</em><em>nn.Dropout</em><em>]</em>) – Dropout layer. Default: None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Output tensor and attention weights.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>tuple[torch.Tensor, torch.Tensor]</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="flexit-callbacks-module">
<h2>flexit.callbacks module<a class="headerlink" href="#flexit-callbacks-module" title="Link to this heading"></a></h2>
<p>Training Callbacks</p>
<p>This module implements callback classes for training events,
including checkpointing and early stopping.</p>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">flexit.callbacks.</span></span><span class="sig-name descname"><span class="pre">Callback</span></span><a class="reference internal" href="_modules/flexit/callbacks.html#Callback"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Base class for training callbacks.</p>
<p>Defines hooks for training events that can be overridden by subclasses.</p>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">on_train_begin</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/callbacks.html#Callback.on_train_begin"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Called at the start of training.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">on_train_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/callbacks.html#Callback.on_train_end"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Called at the end of training.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">on_epoch_begin</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/callbacks.html#Callback.on_epoch_begin"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Called at the start of each epoch.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">on_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/callbacks.html#Callback.on_epoch_end"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Called at the end of each epoch.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">on_train_begin</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">trainer</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/callbacks.html#Callback.on_train_begin"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Called at the start of training.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>trainer</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></span>) – Trainer instance.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">on_train_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">trainer</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/callbacks.html#Callback.on_train_end"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Called at the end of training.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>trainer</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></span>) – Trainer instance.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">on_epoch_begin</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epoch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">trainer</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/callbacks.html#Callback.on_epoch_begin"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Called at the start of each epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>epoch</strong> (<em>int</em>) – Current epoch number.</p></li>
<li><p><strong>trainer</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></span>) – Trainer instance.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">on_epoch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epoch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">trainer</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/callbacks.html#Callback.on_epoch_end"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Called at the end of each epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>epoch</strong> (<em>int</em>) – Current epoch number.</p></li>
<li><p><strong>trainer</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></span>) – Trainer instance.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">flexit.callbacks.</span></span><span class="sig-name descname"><span class="pre">CheckpointCallback</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">save_best</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_last</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">checkpoint_dir</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'checkpoints'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filename_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'checkpoint_epoch_{epoch:03d}.pt'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">best_filename</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'best_model.pt'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/callbacks.html#CheckpointCallback"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Callback</span></code></p>
<p>Callback to handle checkpointing with options to save
only the best model and the last N checkpoints.</p>
<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">save_best</span></span></dt>
<dd><p>Save best model based on validation loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>bool</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">keep_last</span></span></dt>
<dd><p>Keep last N checkpoints.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">checkpoint_dir</span></span></dt>
<dd><p>Directory to save checkpoints.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Path</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">filename_format</span></span></dt>
<dd><p>Format string for checkpoint names.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">best_filename</span></span></dt>
<dd><p>Filename for best model checkpoint.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">best_loss</span></span></dt>
<dd><p>Best validation loss seen so far.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">saved_checkpoints</span></span></dt>
<dd><p>List of saved checkpoint paths.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>list[Path]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">on_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/callbacks.html#CheckpointCallback.on_epoch_end"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Save checkpoint if conditions are met and clean up old checkpoints.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">on_epoch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epoch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">trainer</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/callbacks.html#CheckpointCallback.on_epoch_end"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Save checkpoint if conditions are met and clean up old checkpoints.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>epoch</strong> (<em>int</em>) – Current epoch number.</p></li>
<li><p><strong>trainer</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></span>) – Trainer instance.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span></p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">flexit.callbacks.</span></span><span class="sig-name descname"><span class="pre">EarlyStoppingCallback</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">patience</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/callbacks.html#EarlyStoppingCallback"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Callback</span></code></p>
<p>Callback to stop training early if validation loss doesn’t improve.</p>
<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">patience</span></span></dt>
<dd><p>Number of epochs to wait for improvement.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">min_delta</span></span></dt>
<dd><p>Minimum change in loss to qualify as improvement.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">best_loss</span></span></dt>
<dd><p>Best validation loss seen so far.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">counter</span></span></dt>
<dd><p>Number of epochs since last improvement.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">on_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/callbacks.html#EarlyStoppingCallback.on_epoch_end"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Check if training should stop based on validation loss.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">on_epoch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epoch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">trainer</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/callbacks.html#EarlyStoppingCallback.on_epoch_end"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Check if training should stop based on validation loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>epoch</strong> (<em>int</em>) – Current epoch number.</p></li>
<li><p><strong>trainer</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></span>) – Trainer instance.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span></p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="flexit-configs-module">
<h2>flexit.configs module<a class="headerlink" href="#flexit-configs-module" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">flexit.configs.</span></span><span class="sig-name descname"><span class="pre">ModelConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">src_vocab</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tgt_vocab</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cls_token_id</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">512</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_ff</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2048</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">6</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'encoder-decoder'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pe_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'absolute'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pre_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ff_activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'relu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cpu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'xavier_uniform'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/configs.html#ModelConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Configuration for transformer models.</p>
<p>This dataclass holds configuration settings for various types of transformer models,
including encoder-decoder, encoder-only, and decoder-only architectures.</p>
<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">src_vocab</span></span></dt>
<dd><p>Source vocabulary size.
Required for encoder-decoder or encoder-only models.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">tgt_vocab</span></span></dt>
<dd><p>Target vocabulary size.
Required for encoder-decoder or decoder-only models.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">num_classes</span></span></dt>
<dd><p>Number of classes for classification tasks (e.g., BERT-like).</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">d_model</span></span></dt>
<dd><p>Model dimension. Default is 512.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">d_ff</span></span></dt>
<dd><p>Feed-forward dimension. Default is 2048.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">n_heads</span></span></dt>
<dd><p>Number of attention heads. Default is 8.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">dropout</span></span></dt>
<dd><p>Dropout probability. Default is 0.1.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">n_layers</span></span></dt>
<dd><p>Number of layers. Default is 6.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">model_type</span></span></dt>
<dd><p>Model type. Options are:
- ‘encoder-decoder’
- ‘encoder-only’
- ‘decoder-only’</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">pe_type</span></span></dt>
<dd><p>Positional encoding type. Options are:
- ‘absolute’
- ‘alibi’
- ‘relative’
- ‘rotary’</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">pre_norm</span></span></dt>
<dd><p>Use pre-normalization. Default is True.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">device</span></span></dt>
<dd><p>Device for computation (‘cpu’ or ‘gpu’). Default is ‘cpu’.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">init_method</span></span></dt>
<dd><p>Weight initialization method. Options are:
- ‘xavier_uniform’
- ‘xavier_normal’
- ‘kaiming_uniform’
- ‘kaiming_normal’
- ‘orthogonal’
- ‘zero’
- ‘one’</p>
</dd></dl>

<p class="rubric">Examples</p>
<p>Create a ModelConfig for an encoder-decoder model:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">ModelConfig</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">src_vocab</span><span class="o">=</span><span class="mi">32000</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">tgt_vocab</span><span class="o">=</span><span class="mi">32000</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">n_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">model_type</span><span class="o">=</span><span class="s1">&#39;encoder-decoder&#39;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">pe_type</span><span class="o">=</span><span class="s1">&#39;absolute&#39;</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<p>Create a ModelConfig from a dictionary:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">config_dict</span> <span class="o">=</span> <span class="p">{</span>
<span class="gp">... </span>    <span class="s1">&#39;src_vocab&#39;</span><span class="p">:</span> <span class="mi">32000</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s1">&#39;tgt_vocab&#39;</span><span class="p">:</span> <span class="mi">32000</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s1">&#39;d_model&#39;</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s1">&#39;n_heads&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s1">&#39;model_type&#39;</span><span class="p">:</span> <span class="s1">&#39;encoder-decoder&#39;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s1">&#39;pe_type&#39;</span><span class="p">:</span> <span class="s1">&#39;absolute&#39;</span><span class="p">,</span>
<span class="gp">... </span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">ModelConfig</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">config_dict</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">src_vocab</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code> <span class="pre">|</span> <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">tgt_vocab</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code> <span class="pre">|</span> <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">num_classes</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code> <span class="pre">|</span> <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">cls_token_id</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code> <span class="pre">|</span> <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">d_model</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">512</span></em></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">d_ff</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">2048</span></em></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">n_heads</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">8</span></em></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">dropout</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">0.1</span></em></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">n_layers</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code><span class="pre">[</span><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code><span class="pre">,</span> <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code><span class="pre">]</span> <span class="pre">|</span> <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">6</span></em></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">model_type</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><code class="xref py py-data docutils literal notranslate"><span class="pre">Literal</span></code><span class="pre">[</span><code class="docutils literal notranslate"><span class="pre">'encoder-decoder'</span></code><span class="pre">,</span> <code class="docutils literal notranslate"><span class="pre">'encoder-only'</span></code><span class="pre">,</span> <code class="docutils literal notranslate"><span class="pre">'decoder-only'</span></code><span class="pre">]</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'encoder-decoder'</span></em></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">pe_type</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><code class="xref py py-data docutils literal notranslate"><span class="pre">Literal</span></code><span class="pre">[</span><code class="docutils literal notranslate"><span class="pre">'absolute'</span></code><span class="pre">,</span> <code class="docutils literal notranslate"><span class="pre">'alibi'</span></code><span class="pre">,</span> <code class="docutils literal notranslate"><span class="pre">'relative'</span></code><span class="pre">,</span> <code class="docutils literal notranslate"><span class="pre">'rotary'</span></code><span class="pre">]</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'absolute'</span></em></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">pre_norm</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">True</span></em></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">ff_activation</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><code class="xref py py-data docutils literal notranslate"><span class="pre">Literal</span></code><span class="pre">[</span><code class="docutils literal notranslate"><span class="pre">'relu'</span></code><span class="pre">,</span> <code class="docutils literal notranslate"><span class="pre">'gelu'</span></code><span class="pre">,</span> <code class="docutils literal notranslate"><span class="pre">'tanh'</span></code><span class="pre">,</span> <code class="docutils literal notranslate"><span class="pre">'sigmoid'</span></code><span class="pre">,</span> <code class="docutils literal notranslate"><span class="pre">'leaky_relu'</span></code><span class="pre">,</span> <code class="docutils literal notranslate"><span class="pre">'silu'</span></code><span class="pre">,</span> <code class="docutils literal notranslate"><span class="pre">'elu'</span></code><span class="pre">,</span> <code class="docutils literal notranslate"><span class="pre">'selu'</span></code><span class="pre">]</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'relu'</span></em></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">device</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'cpu'</span></em></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">init_method</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><code class="xref py py-data docutils literal notranslate"><span class="pre">Literal</span></code><span class="pre">[</span><code class="docutils literal notranslate"><span class="pre">'xavier_uniform'</span></code><span class="pre">,</span> <code class="docutils literal notranslate"><span class="pre">'xavier_normal'</span></code><span class="pre">,</span> <code class="docutils literal notranslate"><span class="pre">'kaiming_uniform'</span></code><span class="pre">,</span> <code class="docutils literal notranslate"><span class="pre">'kaiming_normal'</span></code><span class="pre">,</span> <code class="docutils literal notranslate"><span class="pre">'orthogonal'</span></code><span class="pre">,</span> <code class="docutils literal notranslate"><span class="pre">'zero'</span></code><span class="pre">,</span> <code class="docutils literal notranslate"><span class="pre">'one'</span></code><span class="pre">]</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'xavier_uniform'</span></em></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">classmethod</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config_dict</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/configs.html#ModelConfig.from_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Create a ModelConfig instance from a dictionary.</p>
<p>This method filters the input dictionary to include only valid configuration
parameters and creates a new ModelConfig instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>config_dict</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]</span>) – Dictionary containing configuration parameters.
Must contain valid ModelConfig field names.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A new instance initialized with the dictionary values.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>ModelConfig</p>
</dd>
</dl>
<p class="rubric">Example</p>
<p>Creating a config from a dictionary:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">config_dict</span> <span class="o">=</span> <span class="p">{</span>
<span class="gp">... </span>    <span class="s1">&#39;src_vocab&#39;</span><span class="p">:</span> <span class="mi">32000</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s1">&#39;d_model&#39;</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s1">&#39;model_type&#39;</span><span class="p">:</span> <span class="s1">&#39;encoder-decoder&#39;</span><span class="p">,</span>
<span class="gp">... </span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">ModelConfig</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">config_dict</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">flexit.configs.</span></span><span class="sig-name descname"><span class="pre">ConfigDescriptor</span></span><a class="reference internal" href="_modules/flexit/configs.html#ConfigDescriptor"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Descriptor managing model configurations.</p>
<p>This descriptor is used to manage model configurations in the <cite>FlexiTransformer</cite> class.
It allows for easy configuration of the model using keyword arguments.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>value</strong> (<em>dict</em><em> or </em><em>ModelConfig</em>) – The configuration dictionary or ModelConfig instance.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The validated ModelConfig instance.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>ModelConfig</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>AttributeError</strong> – If the configuration is not initialized.</p></li>
<li><p><strong>TypeError</strong> – If the configuration is not a dictionary or ModelConfig instance.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">FlexiTransformer</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">src_vocab</span><span class="o">=</span><span class="mi">32000</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">config</span>
<span class="go">ModelConfig(src_vocab=32000, d_model=512, ...)</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="flexit-core-module">
<h2>flexit.core module<a class="headerlink" href="#flexit-core-module" title="Link to this heading"></a></h2>
<p>Core Transformer Components</p>
<p>This module implements the core components of transformer models,
including encoder, decoder, generator, and the encoder-decoder architecture.</p>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">flexit.core.</span></span><span class="sig-name descname"><span class="pre">EncoderDecoder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">encoder</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decoder</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src_embed</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tgt_embed</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generator</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cpu'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/core.html#EncoderDecoder"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>A standard Encoder-Decoder architecture.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>encoder</strong> (<em>nn.Module</em>) – Encoder module.</p></li>
<li><p><strong>decoder</strong> (<em>nn.Module</em>) – Decoder module.</p></li>
<li><p><strong>src_embed</strong> (<em>nn.Module</em>) – Source embedding module.</p></li>
<li><p><strong>tgt_embed</strong> (<em>nn.Module</em>) – Target embedding module.</p></li>
<li><p><strong>generator</strong> (<em>nn.Module</em>) – Generator module.</p></li>
<li><p><strong>device</strong> (<em>str</em>) – Device for computation (‘cpu’ or ‘gpu’). Default is ‘cpu’.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">encoder</span></span></dt>
<dd><p>Encoder module.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>nn.Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">decoder</span></span></dt>
<dd><p>Decoder module.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>nn.Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">src_embed</span></span></dt>
<dd><p>Source embedding module.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>nn.Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">tgt_embed</span></span></dt>
<dd><p>Target embedding module.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>nn.Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">generator</span></span></dt>
<dd><p>Generator module.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>nn.Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">device</span></span></dt>
<dd><p>Device for computation.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/core.html#EncoderDecoder.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Forward pass for the encoder-decoder model.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">encode</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/core.html#EncoderDecoder.encode"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Encode the source sequence.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">decode</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/core.html#EncoderDecoder.decode"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Decode the target sequence using encoder memory.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/core.html#EncoderDecoder.to"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Move model to specified device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>device</strong> (<em>str</em>) – Target device.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Module instance.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>self</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">src</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tgt</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src_mask</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tgt_mask</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/core.html#EncoderDecoder.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Forward pass for the encoder-decoder model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>src</strong> (<em>torch.Tensor</em>) – Source sequence.</p></li>
<li><p><strong>tgt</strong> (<em>torch.Tensor</em>) – Target sequence.</p></li>
<li><p><strong>src_mask</strong> (<em>torch.Tensor</em>) – Source mask.</p></li>
<li><p><strong>tgt_mask</strong> (<em>torch.Tensor</em>) – Target mask.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Output tensor.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">encode</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">src</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src_mask</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/core.html#EncoderDecoder.encode"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Encode the source sequence.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>src</strong> (<em>torch.Tensor</em>) – Source sequence.</p></li>
<li><p><strong>src_mask</strong> (<em>torch.Tensor</em>) – Source mask.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Encoder output.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">decode</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memory</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src_mask</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tgt</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tgt_mask</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/core.html#EncoderDecoder.decode"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Decode the target sequence using encoder memory.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>memory</strong> (<em>torch.Tensor</em>) – Encoder output.</p></li>
<li><p><strong>src_mask</strong> (<em>torch.Tensor</em>) – Source mask.</p></li>
<li><p><strong>tgt</strong> (<em>torch.Tensor</em>) – Target sequence.</p></li>
<li><p><strong>tgt_mask</strong> (<em>torch.Tensor</em>) – Target mask.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Decoder output.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">flexit.core.</span></span><span class="sig-name descname"><span class="pre">Generator</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vocab</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/core.html#Generator"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Implements the generator (linear + softmax).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>d_model</strong> (<em>int</em>) – Model dimension.</p></li>
<li><p><strong>vocab</strong> (<em>int</em>) – Vocabulary size.</p></li>
<li><p><strong>pre_norm</strong> (<em>bool</em>) – Use pre-normalization. Default is True.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">proj</span></span></dt>
<dd><p>Linear projection layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>nn.Linear</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">norm</span></span></dt>
<dd><p>Layer normalization.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>LayerNorm</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">pre_norm</span></span></dt>
<dd><p>Use pre-normalization.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>bool</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/core.html#Generator.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Forward pass for generator.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/core.html#Generator.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Forward pass for generator.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> (<em>torch.Tensor</em>) – Input tensor.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Output tensor after applying linear projection and softmax.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">flexit.core.</span></span><span class="sig-name descname"><span class="pre">Encoder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_layers</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/core.html#Encoder"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Core encoder is a stack of N layers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>layer</strong> (<em>nn.Module</em>) – Encoder layer module.</p></li>
<li><p><strong>n_layers</strong> (<em>int</em>) – Number of layers.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">layers</span></span></dt>
<dd><p>List of encoder layers.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>nn.ModuleList</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">norm</span></span></dt>
<dd><p>Layer normalization.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>LayerNorm</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">pre_norm</span></span></dt>
<dd><p>Use pre-normalization.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>bool</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/core.html#Encoder.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Forward pass through encoder layers.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/core.html#Encoder.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Pass the input (and mask) through each layer in turn.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>torch.Tensor</em>) – Input tensor.</p></li>
<li><p><strong>mask</strong> (<em>torch.Tensor</em>) – Mask tensor.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Encoder output.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">flexit.core.</span></span><span class="sig-name descname"><span class="pre">Decoder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_layers</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/core.html#Decoder"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Core decoder is a stack of N layers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>layer</strong> (<em>nn.Module</em>) – Decoder layer module.</p></li>
<li><p><strong>n_layers</strong> (<em>int</em>) – Number of layers.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">layers</span></span></dt>
<dd><p>List of decoder layers.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>nn.ModuleList</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">norm</span></span></dt>
<dd><p>Layer normalization.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>LayerNorm</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">pre_norm</span></span></dt>
<dd><p>Use pre-normalization.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>bool</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/core.html#Decoder.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Forward pass through decoder layers.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward_cross_attention</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/core.html#Decoder.forward_cross_attention"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Forward pass with cross-attention.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward_self_attention</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/core.html#Decoder.forward_self_attention"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Forward pass with self-attention.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">memory</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src_mask</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tgt_mask</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/core.html#Decoder.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Router method to appropriate forward implementation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>torch.Tensor</em>) – Input tensor.</p></li>
<li><p><strong>memory</strong> (<em>torch.Tensor</em><em> | </em><em>None</em>) – Encoder memory.</p></li>
<li><p><strong>src_mask</strong> (<em>torch.Tensor</em><em> | </em><em>None</em>) – Source mask.</p></li>
<li><p><strong>tgt_mask</strong> (<em>torch.Tensor</em>) – Target mask.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Decoder output.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward_cross_attention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">memory</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src_mask</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tgt_mask</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/core.html#Decoder.forward_cross_attention"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Forward pass for encoder-decoder with cross-attention.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>torch.Tensor</em>) – Input tensor.</p></li>
<li><p><strong>memory</strong> (<em>torch.Tensor</em>) – Encoder memory.</p></li>
<li><p><strong>src_mask</strong> (<em>torch.Tensor</em>) – Source mask.</p></li>
<li><p><strong>tgt_mask</strong> (<em>torch.Tensor</em>) – Target mask.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Decoder output.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward_self_attention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tgt_mask</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/core.html#Decoder.forward_self_attention"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Forward pass for decoder-only with self-attention.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>torch.Tensor</em>) – Input tensor.</p></li>
<li><p><strong>tgt_mask</strong> (<em>torch.Tensor</em>) – Target mask.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Decoder output.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="flexit-factory-module">
<h2>flexit.factory module<a class="headerlink" href="#flexit-factory-module" title="Link to this heading"></a></h2>
<p>Transformer Factory</p>
<p>This module implements a factory class for creating different types of transformer models
based on configuration parameters.</p>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">flexit.factory.</span></span><span class="sig-name descname"><span class="pre">TransformerFactory</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/factory.html#TransformerFactory"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Factory class for creating transformer models.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>config</strong> (<em>ModelConfig</em>) – Model configuration.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">create_model</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/factory.html#TransformerFactory.create_model"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Create transformer model based on configuration.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">create_model</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/factory.html#TransformerFactory.create_model"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Create transformer model based on configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>Created transformer model.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>nn.Module</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">flexit.factory.</span></span><span class="sig-name descname"><span class="pre">EncoderOnly</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">embed</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">encoder</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">config</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/factory.html#EncoderOnly"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Encoder-only transformer architecture.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>embed</strong> (<em>nn.Module</em>) – Embedding layer.</p></li>
<li><p><strong>encoder</strong> (<em>nn.Module</em>) – Encoder module.</p></li>
<li><p><strong>head</strong> (<em>nn.Module</em>) – Classification head.</p></li>
<li><p><strong>config</strong> (<em>ModelConfig</em>) – Model configuration.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/factory.html#EncoderOnly.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Forward pass through encoder and classification head.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">src</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src_mask</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/factory.html#EncoderOnly.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Forward pass through encoder and classification head.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>src</strong> (<em>torch.Tensor</em>) – Input tensor.</p></li>
<li><p><strong>src_mask</strong> (<em>torch.Tensor</em>) – Mask tensor.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Output tensor.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">flexit.factory.</span></span><span class="sig-name descname"><span class="pre">DecoderOnly</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">embed</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decoder</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generator</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/factory.html#DecoderOnly"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Decoder-only transformer architecture.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>embed</strong> (<em>nn.Module</em>) – Embedding layer.</p></li>
<li><p><strong>decoder</strong> (<em>nn.Module</em>) – Decoder module.</p></li>
<li><p><strong>generator</strong> (<em>nn.Module</em>) – Generator module.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/factory.html#DecoderOnly.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Forward pass through decoder.</p>
</dd></dl>

<p>The implementation allows for both:
1. A simplified interface (tgt, tgt_mask) for decoder-only use
2. The full interface (src, tgt, src_mask, tgt_mask) for compatibility</p>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tgt</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tgt_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/factory.html#DecoderOnly.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Forward pass for the decoder-only model with flexible parameter handling.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tgt</strong> (<em>torch.Tensor</em>) – Target sequence input</p></li>
<li><p><strong>tgt_mask</strong> (<em>torch.Tensor</em><em> | </em><em>None</em>) – Target sequence mask</p></li>
<li><p><strong>src</strong> (<em>torch.Tensor</em><em> | </em><em>None</em>) – Source sequence (unused, kept for interface compatibility)</p></li>
<li><p><strong>src_mask</strong> (<em>torch.Tensor</em><em> | </em><em>None</em>) – Source mask (unused, kept for interface compatibility)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Output tensor</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<ul class="simple">
<li><dl class="simple">
<dt>The src and src_mask parameters are included for interface compatibility</dt><dd><p>but are not used in the decoder-only architecture</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>This implementation allows for both simplified and full interface usage:</dt><dd><p>model(tgt, tgt_mask) or model(src, tgt, src_mask, tgt_mask)</p>
</dd>
</dl>
</li>
</ul>
</dd></dl>

</dd></dl>

</section>
<section id="flexit-layers-module">
<h2>flexit.layers module<a class="headerlink" href="#flexit-layers-module" title="Link to this heading"></a></h2>
<p>Transformer Layers</p>
<p>This module implements fundamental layers used in transformer models,
including layer normalization, sublayer connections, feed-forward networks,
and encoder/decoder layers.</p>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">flexit.layers.</span></span><span class="sig-name descname"><span class="pre">LayerNorm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-06</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/layers.html#LayerNorm"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Construct a Layer Normalization module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>features</strong> (<em>int</em>) – Number of features in the input.</p></li>
<li><p><strong>eps</strong> (<em>float</em>) – A small value to avoid division by zero. Default: 1e-6.</p></li>
<li><p><strong>bias</strong> (<em>bool</em>) – If True, use bias in normalization. Default: True.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">a_2</span></span></dt>
<dd><p>Scaling parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>nn.Parameter</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">b_2</span></span></dt>
<dd><p>Bias parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>nn.Parameter</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">eps</span></span></dt>
<dd><p>Small value for numerical stability.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/layers.html#LayerNorm.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Forward pass through layer normalization.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/layers.html#LayerNorm.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Forward pass through layer normalization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> (<em>torch.Tensor</em>) – Input tensor.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Normalized tensor.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">flexit.layers.</span></span><span class="sig-name descname"><span class="pre">SublayerConnection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pre_norm</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/layers.html#SublayerConnection"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>A residual connection followed by a layer normalization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>size</strong> (<em>int</em>) – Size of the input features.</p></li>
<li><p><strong>pre_norm</strong> (<em>bool</em>) – Use pre-normalization.</p></li>
<li><p><strong>dropout</strong> (<em>float</em>) – Dropout probability.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">norm</span></span></dt>
<dd><p>Normalization block.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>NormalizationBlock</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">pre_norm</span></span></dt>
<dd><p>Use pre-normalization.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>bool</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/layers.html#SublayerConnection.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Forward pass with residual connection.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sublayer</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/layers.html#SublayerConnection.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Apply residual connection to any sublayer with the same size.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>torch.Tensor</em>) – Input tensor.</p></li>
<li><p><strong>sublayer</strong> (<em>Callable</em>) – A sublayer function to apply.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Output tensor after applying the residual connection.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">flexit.layers.</span></span><span class="sig-name descname"><span class="pre">NormalizationBlock</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/layers.html#NormalizationBlock"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Normalization block for pre and post normalization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>size</strong> (<em>int</em>) – Size of the input features.</p></li>
<li><p><strong>dropout</strong> (<em>float</em>) – Dropout probability.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">norm</span></span></dt>
<dd><p>Layer normalization.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>LayerNorm</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">dropout</span></span></dt>
<dd><p>Dropout layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>nn.Dropout</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">pre_normalization</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/layers.html#NormalizationBlock.pre_normalization"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Pre-normalization forward pass.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">post_normalization</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/layers.html#NormalizationBlock.post_normalization"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Post-normalization forward pass.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">pre_normalization</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sublayer</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/layers.html#NormalizationBlock.pre_normalization"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Pre-normalization forward pass.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>torch.Tensor</em>) – Input tensor.</p></li>
<li><p><strong>sublayer</strong> (<em>Callable</em>) – Sublayer function.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Output tensor.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">post_normalization</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sublayer</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/layers.html#NormalizationBlock.post_normalization"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Post-normalization forward pass.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>torch.Tensor</em>) – Input tensor.</p></li>
<li><p><strong>sublayer</strong> (<em>Callable</em>) – Sublayer function.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Output tensor.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">flexit.layers.</span></span><span class="sig-name descname"><span class="pre">PositionwiseFeedForward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_ff</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'relu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/layers.html#PositionwiseFeedForward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Implements the position-wise feed-forward network.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>d_model</strong> (<em>int</em>) – Model dimension.</p></li>
<li><p><strong>d_ff</strong> (<em>int</em>) – Feed-forward dimension.</p></li>
<li><p><strong>dropout</strong> (<em>float</em>) – Dropout probability.</p></li>
<li><p><strong>activation</strong> (<em>Union</em><em>[</em><em>str</em><em>, </em><em>Callable</em><em>[</em><em>[</em><em>Tensor</em><em>]</em><em>, </em><em>Tensor</em><em>]</em><em>]</em>) – Activation function.
Can be a string or callable. Default: ‘relu’.</p></li>
<li><p><strong>bias</strong> (<em>bool</em>) – If True, use bias in Linear layers. Default: True.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">w_1</span></span></dt>
<dd><p>First linear layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>nn.Linear</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">w_2</span></span></dt>
<dd><p>Second linear layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>nn.Linear</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">dropout</span></span></dt>
<dd><p>Dropout layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>nn.Dropout</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">activation</span></span></dt>
<dd><p>Activation function.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/layers.html#PositionwiseFeedForward.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Forward pass through feed-forward network.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/layers.html#PositionwiseFeedForward.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Forward pass through feed-forward network.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> (<em>torch.Tensor</em>) – Input tensor.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Output tensor.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">flexit.layers.</span></span><span class="sig-name descname"><span class="pre">EncoderLayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">self_attn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feed_forward</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pre_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/layers.html#EncoderLayer"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Encoder layer consisting of self-attention and feed-forward layers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>size</strong> (<em>int</em>) – Size of the input features.</p></li>
<li><p><strong>self_attn</strong> (<em>nn.Module</em>) – Self attention module.</p></li>
<li><p><strong>feed_forward</strong> (<em>nn.Module</em>) – Feed-forward module.</p></li>
<li><p><strong>pre_norm</strong> (<em>bool</em>) – Use pre-normalization.</p></li>
<li><p><strong>dropout</strong> (<em>float</em>) – Dropout probability.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">self_attn</span></span></dt>
<dd><p>Self attention module.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>nn.Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">feed_forward</span></span></dt>
<dd><p>Feed-forward module.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>nn.Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">sublayer</span></span></dt>
<dd><p>List of sublayer connections.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>nn.ModuleList</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">size</span></span></dt>
<dd><p>Size of input features.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">pre_norm</span></span></dt>
<dd><p>Use pre-normalization.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>bool</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/layers.html#EncoderLayer.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Forward pass through encoder layer.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/layers.html#EncoderLayer.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Forward pass through encoder layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>torch.Tensor</em>) – Input tensor.</p></li>
<li><p><strong>mask</strong> (<em>torch.Tensor</em>) – Mask tensor.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Output tensor.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">flexit.layers.</span></span><span class="sig-name descname"><span class="pre">DecoderLayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">self_attn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src_attn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feed_forward</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pre_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/layers.html#DecoderLayer"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Decoder layer consisting of self-attention, source-attention, and feed-forward layers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>size</strong> (<em>int</em>) – Size of the input features.</p></li>
<li><p><strong>self_attn</strong> (<em>nn.Module</em>) – Self attention module.</p></li>
<li><p><strong>src_attn</strong> (<em>nn.Module</em>) – Source attention module.</p></li>
<li><p><strong>feed_forward</strong> (<em>nn.Module</em>) – Feed-forward module.</p></li>
<li><p><strong>pre_norm</strong> (<em>bool</em>) – Use pre-normalization.</p></li>
<li><p><strong>dropout</strong> (<em>float</em>) – Dropout probability.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">size</span></span></dt>
<dd><p>Size of input features.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">self_attn</span></span></dt>
<dd><p>Self attention module.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>nn.Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">src_attn</span></span></dt>
<dd><p>Source attention module.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>nn.Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">feed_forward</span></span></dt>
<dd><p>Feed-forward module.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>nn.Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">sublayer</span></span></dt>
<dd><p>List of sublayer connections.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>nn.ModuleList</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">pre_norm</span></span></dt>
<dd><p>Use pre-normalization.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>bool</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/layers.html#DecoderLayer.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Forward pass through decoder layer.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">memory</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src_mask</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tgt_mask</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/layers.html#DecoderLayer.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Forward pass for the decoder layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>torch.Tensor</em>) – Input tensor.</p></li>
<li><p><strong>memory</strong> (<em>torch.Tensor</em>) – Memory tensor from the encoder.</p></li>
<li><p><strong>src_mask</strong> (<em>torch.Tensor</em>) – Source mask tensor.</p></li>
<li><p><strong>tgt_mask</strong> (<em>torch.Tensor</em>) – Target mask tensor.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Output tensor.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">flexit.layers.</span></span><span class="sig-name descname"><span class="pre">Embeddings</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vocab</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/layers.html#Embeddings"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Implements token embeddings.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>d_model</strong> (<em>int</em>) – Model dimension.</p></li>
<li><p><strong>vocab</strong> (<em>int</em>) – Vocabulary size.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">lut</span></span></dt>
<dd><p>Embedding layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>nn.Embedding</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">d_model</span></span></dt>
<dd><p>Model dimension.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/layers.html#Embeddings.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Forward pass through embedding layer.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/layers.html#Embeddings.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Forward pass through embedding layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> (<em>torch.Tensor</em>) – Input tensor.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Embedded tensor.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="flexit-loss-module">
<h2>flexit.loss module<a class="headerlink" href="#flexit-loss-module" title="Link to this heading"></a></h2>
<p>Loss Functions</p>
<p>This module implements various loss functions used in transformer training,
including label smoothing and enhanced loss computation with gradient clipping.</p>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">flexit.loss.</span></span><span class="sig-name descname"><span class="pre">LabelSmoothing</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">smoothing</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/loss.html#LabelSmoothing"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Implement label smoothing.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>size</strong> (<em>int</em>) – Vocabulary size.</p></li>
<li><p><strong>padding_idx</strong> (<em>int</em>) – Padding token index.</p></li>
<li><p><strong>smoothing</strong> (<em>float</em>) – Smoothing value. Default: 0.0.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">criterion</span></span></dt>
<dd><p>Loss criterion.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>nn.KLDivLoss</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">padding_idx</span></span></dt>
<dd><p>Padding token index.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">confidence</span></span></dt>
<dd><p>Confidence value.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">smoothing</span></span></dt>
<dd><p>Smoothing value.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">size</span></span></dt>
<dd><p>Vocabulary size.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">true_dist</span></span></dt>
<dd><p>True distribution.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/loss.html#LabelSmoothing.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Forward pass through label smoothing.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/loss.html#LabelSmoothing.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Forward pass through label smoothing.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>torch.Tensor</em>) – Model output logits.</p></li>
<li><p><strong>target</strong> (<em>torch.Tensor</em>) – Target labels.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Loss value.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">flexit.loss.</span></span><span class="sig-name descname"><span class="pre">LossCompute</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">generator</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">criterion</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_clip</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/loss.html#LossCompute"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Enhanced loss computation with proper normalization and gradient handling.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>generator</strong> (<em>nn.Module</em>) – Model’s output generator</p></li>
<li><p><strong>criterion</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Callable</span></code></span>) – Loss criterion (typically CrossEntropyLoss or KLDivLoss)</p></li>
<li><p><strong>grad_clip</strong> (<em>float</em><em>, </em><em>optional</em>) – Maximum norm for gradient clipping</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">flexit.loss.</span></span><span class="sig-name descname"><span class="pre">BertLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">grad_clip</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/loss.html#BertLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Enhanced BERT-style loss computation with proper scaling.</p>
</dd></dl>

</section>
<section id="flexit-models-module">
<h2>flexit.models module<a class="headerlink" href="#flexit-models-module" title="Link to this heading"></a></h2>
<p>Transformer Models</p>
<p>This module implements various transformer model variants,
including encoder-decoder, encoder-only, and decoder-only architectures.</p>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">flexit.models.</span></span><span class="sig-name descname"><span class="pre">BaseTransformer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/models.html#BaseTransformer"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Generic</span></code>[<code class="xref py py-obj docutils literal notranslate"><span class="pre">T</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Base transformer that initializes configuration and model building.
This class serves as a base implementation for transformer models, handling configuration
initialization and model construction. It provides a generic interface for encoder-decoder,
encoder-only, and decoder-only transformer architectures.</p>
<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">config</span></span></dt>
<dd><p>Configuration object containing model parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>ModelConfig</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">factory</span></span></dt>
<dd><p>Factory class for creating transformer models.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>TransformerFactory</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">_base_model</span></span></dt>
<dd><p>The underlying transformer model instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>nn.Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">generator</span></span></dt>
<dd><p>Output generator module of the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>nn.Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">encoder</span></span></dt>
<dd><p>Encoder module if present in the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>nn.Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">decoder</span></span></dt>
<dd><p>Decoder module if present in the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>nn.Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">Example</span> <span class="pre">usage</span></span></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">&gt;&gt;&gt;</span> <span class="pre">config</span> <span class="pre">=</span> <span class="pre">{</span></span></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">...</span>&#160;&#160;&#160;&#160; <span class="pre">'model_type'</span></span></dt>
<dd><p>‘encoder-decoder’,</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">...</span>&#160;&#160;&#160;&#160; <span class="pre">'src_vocab'</span></span></dt>
<dd><p>1000,</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">...</span>&#160;&#160;&#160;&#160; <span class="pre">'tgt_vocab'</span></span></dt>
<dd><p>1000,</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">...</span> <span class="pre">}</span></span></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">&gt;&gt;&gt;</span> <span class="pre">transformer</span> <span class="pre">=</span> <span class="pre">(</span></span></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">...</span>&#160;&#160;&#160;&#160; <span class="pre">BaseTransformer(</span></span></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">...</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">config</span></span></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">...</span>&#160;&#160;&#160;&#160; <span class="pre">)</span></span></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">...</span> <span class="pre">)</span></span></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">&gt;&gt;&gt;</span> <span class="pre">output</span> <span class="pre">=</span> <span class="pre">(</span></span></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">...</span>&#160;&#160;&#160;&#160; <span class="pre">transformer(</span></span></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">...</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">input_ids</span></span></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">...</span>&#160;&#160;&#160;&#160; <span class="pre">)</span></span></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">...</span> <span class="pre">)</span></span></dt>
<dd></dd></dl>

<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This class uses a ConfigDescriptor for managing model configuration and provides
flexible initialization through either a configuration dictionary/object or
individual parameter settings.</p>
</div>
<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">config</span></span></dt>
<dd><p>Descriptor managing model configurations.</p>
<p>This descriptor is used to manage model configurations in the <cite>FlexiTransformer</cite> class.
It allows for easy configuration of the model using keyword arguments.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>value</strong> (<em>dict</em><em> or </em><em>ModelConfig</em>) – The configuration dictionary or ModelConfig instance.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The validated ModelConfig instance.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>ModelConfig</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>AttributeError</strong> – If the configuration is not initialized.</p></li>
<li><p><strong>TypeError</strong> – If the configuration is not a dictionary or ModelConfig instance.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">FlexiTransformer</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">src_vocab</span><span class="o">=</span><span class="mi">32000</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">config</span>
<span class="go">ModelConfig(src_vocab=32000, d_model=512, ...)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">encode</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/models.html#BaseTransformer.encode"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Forwards the encode method call to the underlying model.
This method attempts to call the ‘encode’ method on the underlying model instance.
If the model does not have an encode method, it raises an AttributeError.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*args</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></span>) – Variable length argument list to pass to model’s encode method</p></li>
<li><p><strong>**kwargs</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></span>) – Arbitrary keyword arguments to pass to model’s encode method</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The encoded output from the model’s encode method</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>AttributeError</strong> – If the underlying model does not have an encode method</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">decode</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/models.html#BaseTransformer.decode"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Decodes the model’s output.
This method attempts to call the decode method of the underlying model if it exists.
If the model doesn’t have a decode method, it raises an AttributeError.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*args</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></span>) – Variable length argument list to pass to model’s decode method.</p></li>
<li><p><strong>**kwargs</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></span>) – Arbitrary keyword arguments to pass to model’s decode method.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span></p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The decoded output from the model’s decode method.</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>AttributeError</strong> – If the underlying model doesn’t have a decode method.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/models.html#BaseTransformer.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Forward pass of the model wrapper.
This method performs a forward pass by delegating to the underlying model’s forward method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*args</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></span>) – Variable length argument list to be passed to the underlying model.</p></li>
<li><p><strong>**kwargs</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></span>) – Arbitrary keyword arguments to be passed to the underlying model.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The output tensor from the model’s forward pass.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">save</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/models.html#BaseTransformer.save"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Save the configuration and model state.</p>
<p>Saves the model’s configuration and state dictionary to a file at the specified path.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>path</strong> (<em>str</em>) – The file path where the model and configuration will be saved.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span></p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span>
<span class="gp">... </span>    <span class="s1">&#39;model_checkpoint.pt&#39;</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">load</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/models.html#BaseTransformer.load"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Load configuration and model state from file.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>path</strong> (<em>str</em>) – Path to the checkpoint file containing model configuration and state.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span></p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>None</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>FileNotFoundError</strong> – If the checkpoint file does not exist.</p></li>
<li><p><strong>RuntimeError</strong> – If the checkpoint file is corrupted or incompatible.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">flexit.models.</span></span><span class="sig-name descname"><span class="pre">FlexiTransformer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/models.html#FlexiTransformer"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">BaseTransformer</span></code></p>
<p>This class provides a flexible implementation that can be configured for different
transformer architectures including BERT-style encoder-only models, GPT-style decoder-only
models, and full encoder-decoder transformer models.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>src_vocab</strong> (<em>int</em><em>, </em><em>optional</em>) – Size of source vocabulary for encoder. Required for encoder-only
and encoder-decoder models.</p></li>
<li><p><strong>tgt_vocab</strong> (<em>int</em><em>, </em><em>optional</em>) – Size of target vocabulary for decoder. Required for decoder-only
and encoder-decoder models.</p></li>
<li><p><strong>n_layers</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of transformer layers. Defaults to model’s default setting.</p></li>
<li><p><strong>n_heads</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of attention heads. Defaults to model’s default setting.</p></li>
<li><p><strong>**kwargs</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></span>) – Additional keyword arguments passed to parent BaseTransformer class.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<p>Encoder-only (BERT-style):
Decoder-only (GPT-style):
Encoder-Decoder (Transformer-style):</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This class uses a ConfigDescriptor for managing model configuration and provides
flexible initialization through either a configuration dictionary/object or
individual parameter settings.</p>
</div>
<p>usage:
&gt;&gt;&gt; config = {
…     ‘model_type’: ‘encoder-decoder’,
…     ‘src_vocab’: 1000,
…     ‘tgt_vocab’: 1000,
…     ‘d_model’: 768,
…     ‘n_heads’: 12,
…     ‘n_layers’: 12,
…     ‘pe_type’: ‘absolute’,
…     ‘init_method’: ‘xavier_uniform’,
…     ‘pre_norm’: True,
… }</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span> <span class="o">=</span> <span class="n">FlexiTransformer</span><span class="p">(</span>
<span class="gp">... </span>    <span class="o">**</span><span class="n">config</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="p">(</span>
<span class="gp">... </span>    <span class="n">transformer</span><span class="p">(</span>
<span class="gp">... </span>        <span class="n">input_ids</span>
<span class="gp">... </span>    <span class="p">)</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
<span class="gp">... </span>    <span class="s1">&#39;model_type&#39;</span><span class="p">:</span> <span class="s1">&#39;encoder-only&#39;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s1">&#39;src_vocab&#39;</span><span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s1">&#39;d_model&#39;</span><span class="p">:</span> <span class="mi">768</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s1">&#39;n_heads&#39;</span><span class="p">:</span> <span class="mi">12</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s1">&#39;n_layers&#39;</span><span class="p">:</span> <span class="mi">12</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s1">&#39;pe_type&#39;</span><span class="p">:</span> <span class="s1">&#39;absolute&#39;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s1">&#39;init_method&#39;</span><span class="p">:</span> <span class="s1">&#39;xavier_uniform&#39;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s1">&#39;pre_norm&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
<span class="gp">... </span><span class="p">}</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">BERT</span> <span class="o">=</span> <span class="n">FlexiTransformer</span><span class="p">(</span>
<span class="gp">... </span>    <span class="o">**</span><span class="n">config</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="p">(</span>
<span class="gp">... </span>    <span class="n">transformer</span><span class="p">(</span>
<span class="gp">... </span>        <span class="n">input_ids</span>
<span class="gp">... </span>    <span class="p">)</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
<span class="gp">... </span>    <span class="s1">&#39;model_type&#39;</span><span class="p">:</span> <span class="s1">&#39;decoder-only&#39;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s1">&#39;tgt_vocab&#39;</span><span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s1">&#39;d_model&#39;</span><span class="p">:</span> <span class="mi">768</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s1">&#39;n_heads&#39;</span><span class="p">:</span> <span class="mi">12</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s1">&#39;n_layers&#39;</span><span class="p">:</span> <span class="mi">12</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s1">&#39;pe_type&#39;</span><span class="p">:</span> <span class="s1">&#39;absolute&#39;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s1">&#39;init_method&#39;</span><span class="p">:</span> <span class="s1">&#39;xavier_uniform&#39;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s1">&#39;pre_norm&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
<span class="gp">... </span><span class="p">}</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">GPT</span> <span class="o">=</span> <span class="n">FlexiTransformer</span><span class="p">(</span>
<span class="gp">... </span>    <span class="o">**</span><span class="n">config</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="p">(</span>
<span class="gp">... </span>    <span class="n">transformer</span><span class="p">(</span>
<span class="gp">... </span>        <span class="n">input_ids</span>
<span class="gp">... </span>    <span class="p">)</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">flexit.models.</span></span><span class="sig-name descname"><span class="pre">TransformerModel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">src_vocab</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tgt_vocab</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">512</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">6</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pe_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'absolute'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'xavier_uniform'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pre_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/models.html#TransformerModel"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">FlexiTransformer</span></code></p>
<p>A flexible implementation of the Transformer architecture.</p>
<p>This class implements a configurable Transformer model that can be adapted for
various sequence-to-sequence tasks. It extends the FlexiTransformer base class
with specific configurations for a standard Transformer architecture.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>src_vocab</strong> (<em>int</em>) – Size of the source vocabulary.</p></li>
<li><p><strong>tgt_vocab</strong> (<em>int</em>) – Size of the target vocabulary.</p></li>
<li><p><strong>d_model</strong> (<em>int</em><em>, </em><em>optional</em>) – Dimension of the model’s hidden states. Defaults to 512.</p></li>
<li><p><strong>n_heads</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of attention heads in multi-head attention layers.
Defaults to 8.</p></li>
<li><p><strong>n_layers</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of encoder and decoder layers. Defaults to 6.</p></li>
<li><p><strong>pe_type</strong> (<em>str</em><em>, </em><em>optional</em>) – Type of positional encoding to use (‘absolute’ or ‘relative’).
Defaults to ‘absolute’.</p></li>
<li><p><strong>init_method</strong> (<em>str</em><em>, </em><em>optional</em>) – Weight initialization method. Defaults to ‘xavier_uniform’.</p></li>
<li><p><strong>pre_norm</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, uses pre-layer normalization architecture.
If False, uses post-layer normalization. Defaults to True.</p></li>
<li><p><strong>**kwargs</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></span>) – Additional arguments passed to the parent FlexiTransformer class.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The model uses identical number of layers for both encoder and decoder parts.
For different encoder/decoder depths, use the base FlexiTransformer class directly.</p>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">flexit.models.</span></span><span class="sig-name descname"><span class="pre">FlexiBERT</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">src_vocab</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">512</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">6</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pe_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'alibi'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'xavier_uniform'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pre_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/models.html#FlexiBERT"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">BaseTransformer</span></code></p>
<p>A flexible BERT-style transformer implementation that can be configured for various tasks.
This class extends BaseTransformer to provide a configurable BERT-like architecture
with flexible positional encoding, normalization, and initialization options.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>src_vocab</strong> (<em>int</em>) – Size of source vocabulary.</p></li>
<li><p><strong>num_classes</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of output classes for classification. Defaults to 2.</p></li>
<li><p><strong>d_model</strong> (<em>int</em><em>, </em><em>optional</em>) – Dimension of model embeddings. Defaults to 512.</p></li>
<li><p><strong>n_heads</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of attention heads. Defaults to 8.</p></li>
<li><p><strong>n_layers</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of transformer layers. Defaults to 6.</p></li>
<li><p><strong>pe_type</strong> (<em>str</em><em>, </em><em>optional</em>) – Type of positional encoding to use. Defaults to ‘alibi’.</p></li>
<li><p><strong>init_method</strong> (<em>str</em><em>, </em><em>optional</em>) – Weight initialization method. Defaults to ‘xavier_uniform’.</p></li>
<li><p><strong>pre_norm</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, uses pre-norm architecture variant. Defaults to True.</p></li>
<li><p><strong>**kwargs</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></span>) – Additional keyword arguments passed to BaseTransformer.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">reconfigure_head(new_head</span></span></dt>
<dd><p>nn.Module) -&gt; None:
Replaces the classification head with a new module.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">__call__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Forward pass through the model. Delegates to internal model instance.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">reconfigure_head</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">new_head</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/models.html#FlexiBERT.reconfigure_head"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Replace the classification head of the model with a new one.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>new_head</strong> (<em>nn.Module</em>) – New classification head module to replace the existing one.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span></p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">reconfigure_head</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">new_head</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
<span class="gp">... </span>        <span class="mi">512</span><span class="p">,</span> <span class="mi">10</span>
<span class="gp">... </span>    <span class="p">)</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">flexit.models.</span></span><span class="sig-name descname"><span class="pre">FlexiGPT</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tgt_vocab</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">512</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">6</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pe_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'rotary'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'xavier_uniform'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pre_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/models.html#FlexiGPT"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">BaseTransformer</span></code></p>
<p>A flexible GPT-style transformer implementation that can be configured for various tasks.
This class implements a GPT-style decoder-only transformer that can be customized for
different language modeling tasks. It inherits from BaseTransformer and provides a
configurable architecture through various parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tgt_vocab</strong> (<em>int</em>) – Size of target vocabulary.</p></li>
<li><p><strong>d_model</strong> (<em>int</em><em>, </em><em>optional</em>) – Dimension of model embeddings and hidden states. Defaults to 512.</p></li>
<li><p><strong>n_heads</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of attention heads in each layer. Defaults to 8.</p></li>
<li><p><strong>n_layers</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of transformer layers. Defaults to 6.</p></li>
<li><p><strong>pe_type</strong> (<em>str</em><em>, </em><em>optional</em>) – Type of positional encoding to use (‘rotary’, ‘absolute’, etc).
Defaults to ‘rotary’.</p></li>
<li><p><strong>init_method</strong> (<em>str</em><em>, </em><em>optional</em>) – Weight initialization method. Defaults to ‘xavier_uniform’.</p></li>
<li><p><strong>pre_norm</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to use pre-layer normalization. Defaults to True.</p></li>
<li><p><strong>**kwargs</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></span>) – Additional keyword arguments passed to the base transformer.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">FlexiGPT</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">tgt_vocab</span><span class="o">=</span><span class="mi">50000</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">d_model</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">n_heads</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">input_ids</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="flexit-models-heads-module">
<h2>flexit.models_heads module<a class="headerlink" href="#flexit-models-heads-module" title="Link to this heading"></a></h2>
<p>Model Heads</p>
<p>This module implements various model heads for different transformer architectures,
including decoding strategies and classification heads.</p>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">flexit.models_heads.</span></span><span class="sig-name descname"><span class="pre">DecoderStrategy</span></span><a class="reference internal" href="_modules/flexit/models_heads.html#DecoderStrategy"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Base class for decoder strategies.</p>
<p>This class defines the interface for decoding strategies used in transformer models.</p>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">decode</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/models_heads.html#DecoderStrategy.decode"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Static method to perform decoding using a specific strategy.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">static</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">decode</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src_mask</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">start_symbol</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/models_heads.html#DecoderStrategy.decode"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">flexit.models_heads.</span></span><span class="sig-name descname"><span class="pre">EncoderDecoderStrategy</span></span><a class="reference internal" href="_modules/flexit/models_heads.html#EncoderDecoderStrategy"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">DecoderStrategy</span></code></p>
<p>Decoding strategy for encoder-decoder models.</p>
<p>This strategy uses the encoder-decoder architecture for decoding.</p>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">decode</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/models_heads.html#EncoderDecoderStrategy.decode"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Perform decoding using encoder-decoder architecture.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">static</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">decode</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src_mask</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">start_symbol</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/models_heads.html#EncoderDecoderStrategy.decode"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Perform decoding using the specified strategy.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></span>) – Transformer model.</p></li>
<li><p><strong>src</strong> (<em>torch.Tensor</em>) – Source sequence.</p></li>
<li><p><strong>src_mask</strong> (<em>torch.Tensor</em>) – Source mask.</p></li>
<li><p><strong>max_len</strong> (<em>int</em>) – Maximum length for decoding.</p></li>
<li><p><strong>start_symbol</strong> (<em>int</em>) – Start symbol for decoding.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Decoded sequence.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">flexit.models_heads.</span></span><span class="sig-name descname"><span class="pre">DecoderOnlyStrategy</span></span><a class="reference internal" href="_modules/flexit/models_heads.html#DecoderOnlyStrategy"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">DecoderStrategy</span></code></p>
<p>Decoding strategy for decoder-only models.</p>
<p>This strategy uses the decoder-only architecture for decoding.</p>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">decode</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/models_heads.html#DecoderOnlyStrategy.decode"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Perform decoding using decoder-only architecture.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">static</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">decode</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src_mask</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">start_symbol</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/models_heads.html#DecoderOnlyStrategy.decode"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Perform decoding using decoder-only architecture.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>nn.Module</em>) – Transformer model.</p></li>
<li><p><strong>src</strong> (<em>torch.Tensor</em>) – Source sequence.</p></li>
<li><p><strong>src_mask</strong> (<em>torch.Tensor</em><em> | </em><em>None</em>) – Source mask.</p></li>
<li><p><strong>max_len</strong> (<em>int</em>) – Maximum length for decoding.</p></li>
<li><p><strong>start_symbol</strong> (<em>int</em>) – Start symbol for decoding.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Decoded sequence.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">flexit.models_heads.</span></span><span class="sig-name descname"><span class="pre">greedy_decode</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src_mask</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">start_symbol</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/models_heads.html#greedy_decode"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Greedy decoding function.</p>
<p>This function selects the appropriate decoding strategy based on the model type.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></span>) – Transformer model.</p></li>
<li><p><strong>src</strong> (<em>torch.Tensor</em>) – Source sequence.</p></li>
<li><p><strong>src_mask</strong> (<em>torch.Tensor</em>) – Source mask.</p></li>
<li><p><strong>max_len</strong> (<em>int</em>) – Maximum length for decoding.</p></li>
<li><p><strong>start_symbol</strong> (<em>int</em>) – Start symbol for decoding.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Decoded sequence.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – If model type is not supported.</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">flexit.models_heads.</span></span><span class="sig-name descname"><span class="pre">BertHead</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_classes</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pre_norm=True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout=0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation=&lt;built-in</span> <span class="pre">function</span> <span class="pre">gelu&gt;</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/models_heads.html#BertHead"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>BERT-style classification head for encoder-only models.</p>
<p>This implementation follows the standard BERT approach:
1. Takes the [CLS] token representation (first token)
2. Applies a transformation with LayerNorm
3. Projects to the target number of classes</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>d_model</strong> (<em>int</em>) – Hidden dimension of the transformer model</p></li>
<li><p><strong>num_classes</strong> (<em>int</em>) – Number of output classes</p></li>
<li><p><strong>dropout</strong> (<em>float</em><em>, </em><em>optional</em>) – Dropout probability. Default: 0.1</p></li>
<li><p><strong>activation</strong> (<em>callable</em><em>, </em><em>optional</em>) – Activation function. Default: torch.tanh</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">dense</span></span></dt>
<dd><p>Linear layer for transformation.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>nn.Linear</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">activation</span></span></dt>
<dd><p>Activation function.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">norm</span></span></dt>
<dd><p>Layer normalization.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>LayerNorm</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">dropout</span></span></dt>
<dd><p>Dropout layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>nn.Dropout</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">classifier</span></span></dt>
<dd><p>Classification layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>nn.Linear</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/models_heads.html#BertHead.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Forward pass through the classification head.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_states</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/models_heads.html#BertHead.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Forward pass for BERT classification head.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>hidden_states</strong> (<em>torch.Tensor</em>) – Output from the transformer encoder.
Expected shape: [batch_size, seq_len, d_model]</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Classification logits with shape [batch_size, num_classes]</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="flexit-pos-embeddings-module">
<h2>flexit.pos_embeddings module<a class="headerlink" href="#flexit-pos-embeddings-module" title="Link to this heading"></a></h2>
<p>Positional Encoding Implementations</p>
<p>This module implements various positional encoding methods used in transformer models,
including absolute, rotary, and ALiBi positional encodings.</p>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">flexit.pos_embeddings.</span></span><span class="sig-name descname"><span class="pre">AbsolutePositionalEncoding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5000</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/pos_embeddings.html#AbsolutePositionalEncoding"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Standard sinusoidal positional encoding.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>d_model</strong> (<em>int</em>) – Model dimension.</p></li>
<li><p><strong>dropout</strong> (<em>float</em>) – Dropout probability.</p></li>
<li><p><strong>max_len</strong> (<em>int</em>) – Maximum sequence length. Default: 5000.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">dropout</span></span></dt>
<dd><p>Dropout layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>nn.Dropout</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">pe</span></span></dt>
<dd><p>Positional encoding tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/pos_embeddings.html#AbsolutePositionalEncoding.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Add positional encoding to input tensor.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/pos_embeddings.html#AbsolutePositionalEncoding.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Add positional encoding to input tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> (<em>torch.Tensor</em>) – Input tensor.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Tensor with positional encoding added.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">flexit.pos_embeddings.</span></span><span class="sig-name descname"><span class="pre">RotaryPositionalEncoding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10000</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/pos_embeddings.html#RotaryPositionalEncoding"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Implementation of rotary positional encoding as described in
<a class="reference external" href="https://arxiv.org/abs/2104.09864">RoFormer: Enhanced Transformer with Rotary Position Embedding</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>d_features</strong> (<em>int</em>) – Dimension of features to apply rotary encoding to.</p></li>
<li><p><strong>base</strong> (<em>int</em>) – Base for frequency bands calculation. Default: 10000.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">d_features</span></span></dt>
<dd><p>Dimension of features.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">base</span></span></dt>
<dd><p>Base for frequency bands calculation.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">cos_cache</span></span></dt>
<dd><p>Cached cosine values.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">sin_cache</span></span></dt>
<dd><p>Cached sine values.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">_build_cache</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/pos_embeddings.html#RotaryPositionalEncoding._build_cache"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Build cache for rotary encoding.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">_negative_half</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/pos_embeddings.html#RotaryPositionalEncoding._negative_half"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Create negative half of features.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/pos_embeddings.html#RotaryPositionalEncoding.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Apply rotary positional encoding to input tensor.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/pos_embeddings.html#RotaryPositionalEncoding.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Compute the rotary positional encoding of the input tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> (<em>torch.Tensor</em>) – Input tensor.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The sin and cos of the rotary positional encoding.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>tuple[torch.Tensor, torch.Tensor]</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">flexit.pos_embeddings.</span></span><span class="sig-name descname"><span class="pre">ALiBiPositionalEncoding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5000</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/pos_embeddings.html#ALiBiPositionalEncoding"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Implements ALiBi (Attention with Linear Biases) positional encoding.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_heads</strong> (<em>int</em>) – Number of attention heads.</p></li>
<li><p><strong>max_len</strong> (<em>int</em>) – Maximum sequence length. Default: 5000.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">num_heads</span></span></dt>
<dd><p>Number of attention heads.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">max_len</span></span></dt>
<dd><p>Maximum sequence length.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">slopes</span></span></dt>
<dd><p>Slopes for each attention head.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">_get_slopes</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/pos_embeddings.html#ALiBiPositionalEncoding._get_slopes"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Calculate slopes for each attention head.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/pos_embeddings.html#ALiBiPositionalEncoding.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Generate attention biases for ALiBi.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">seq_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/pos_embeddings.html#ALiBiPositionalEncoding.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Generate attention biases for ALiBi.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>seq_len</strong> (<em>int</em>) – Sequence length.</p></li>
<li><p><strong>device</strong> (<em>torch.device</em>) – Device to store the biases.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Attention biases for ALiBi.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="flexit-train-module">
<h2>flexit.train module<a class="headerlink" href="#flexit-train-module" title="Link to this heading"></a></h2>
<p>This module implements training utilities including batch processing,
learning rate scheduling, training state tracking, and the main training loop.</p>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">flexit.train.</span></span><span class="sig-name descname"><span class="pre">Batch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">src</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tgt</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cpu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'encoder-decoder'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/train.html#Batch"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Unified batch handling for all transformer architectures.
Handles encoder-decoder, decoder-only, and encoder-only (BERT) models.</p>
<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">src</span></span></dt>
<dd><p>Source sequence.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">tgt</span></span></dt>
<dd><p>Target sequence.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">labels</span></span></dt>
<dd><p>Classification labels.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">src_mask</span></span></dt>
<dd><p>Source mask.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">tgt_mask</span></span></dt>
<dd><p>Target mask.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">ntokens</span></span></dt>
<dd><p>Number of tokens in the batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">model_type</span></span></dt>
<dd><p>Type of transformer architecture.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Literal</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">device</span></span></dt>
<dd><p>Computation device.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">pad</span></span></dt>
<dd><p>Padding token ID.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">_validate_inputs</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/train.html#Batch._validate_inputs"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Validate input tensors based on model type.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">__init_encoder_only</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Initialize for encoder-only models.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">__init_decoder_only</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Initialize for decoder-only models.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">__init_decoder_decoder</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Initialize for encoder-decoder models.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/train.html#Batch.to"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Move batch to device.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">static</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">make_std_mask</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tgt</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/train.html#Batch.make_std_mask"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Create a mask to hide padding and future words.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tgt</strong> (<em>torch.Tensor</em>) – Target sequence</p></li>
<li><p><strong>pad</strong> (<em>int</em>) – Padding token index</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Target mask</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/train.html#Batch.to"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Move batch to device.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Batch</span></code></span></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">flexit.train.</span></span><span class="sig-name descname"><span class="pre">lr_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">step</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">factor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warmup</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/train.html#lr_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Compute the learning rate based on the step, model size, factor, and warmup.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>step</strong> (<em>int</em>) – Current step.</p></li>
<li><p><strong>model_size</strong> (<em>int</em>) – Model dimension.</p></li>
<li><p><strong>factor</strong> (<em>float</em>) – Scaling factor.</p></li>
<li><p><strong>warmup</strong> (<em>int</em>) – Warmup steps.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Computed learning rate.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">flexit.train.</span></span><span class="sig-name descname"><span class="pre">create_progress_bar</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/train.html#create_progress_bar"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Create an enhanced progress bar with comprehensive metrics.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Progress</span></code></span></p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">flexit.train.</span></span><span class="sig-name descname"><span class="pre">TrainState</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">save_dir</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/train.html#TrainState"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Track number of steps, examples, and tokens processed</p>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ntokens</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/train.html#TrainState.update"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Update training state with batch statistics.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch_size</strong> (<em>int</em>) – Number of samples in the batch</p></li>
<li><p><strong>ntokens</strong> (<em>int</em>) – Number of tokens in the batch</p></li>
<li><p><strong>loss</strong> (<em>float</em>) – Loss value for the batch</p></li>
<li><p><strong>lr</strong> (<em>float</em>) – Current learning rate</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The updated training state instance</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>TrainState</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">save</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/train.html#TrainState.save"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Save training state to a file</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">flexit.train.</span></span><span class="sig-name descname"><span class="pre">run_epoch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data_iter</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_compute</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scheduler</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'train'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">accum_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_batches</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cuda'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_dir</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/train.html#run_epoch"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Enhanced training/evaluation pipeline with comprehensive monitoring.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data_iter</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code></span>) – Data iterator</p></li>
<li><p><strong>model</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></span>) – Neural network model</p></li>
<li><p><strong>loss_compute</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Callable</span></code></span>) – Loss computation function</p></li>
<li><p><strong>optimizer</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code></span>) – Optimization algorithm</p></li>
<li><p><strong>scheduler</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">_LRScheduler</span></code></span>) – Learning rate scheduler</p></li>
<li><p><strong>mode</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Literal</span></code>[<code class="docutils literal notranslate"><span class="pre">'train'</span></code>, <code class="docutils literal notranslate"><span class="pre">'eval'</span></code>, <code class="docutils literal notranslate"><span class="pre">'train+log'</span></code>]</span>) – Training mode (‘train’, ‘eval’, ‘train+log’)</p></li>
<li><p><strong>accum_iter</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – Gradient accumulation steps</p></li>
<li><p><strong>train_state</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">TrainState</span></code> | <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span>) – Training state tracker</p></li>
<li><p><strong>device</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></span>) – Computation device</p></li>
<li><p><strong>save_dir</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Path</span></code> | <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span>) – Directory to save metrics</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>(average_loss, updated_metrics)</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>tuple[float, TrainingState]</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">flexit.train.</span></span><span class="sig-name descname"><span class="pre">TrainerMetrics</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epochs=&lt;factory&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_losses=&lt;factory&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">val_losses=&lt;factory&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_times=&lt;factory&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rates=&lt;factory&gt;</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/train.html#TrainerMetrics"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Track and manage training metrics across epochs.</p>
<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">epochs</span></span></dt>
<dd><p>List of epochs.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>list[int]</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">train_losses</span></span></dt>
<dd><p>Training losses recorded for each epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>list[float]</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">val_losses</span></span></dt>
<dd><p>Validation losses recorded for each epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>list[float]</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">train_times</span></span></dt>
<dd><p>Time taken for training in each epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>list[float]</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">learning_rates</span></span></dt>
<dd><p>Learning rates used in each epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>list[float]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_loss</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">val_loss</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epoch_time</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epoch</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/train.html#TrainerMetrics.update"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Update metrics with the latest epoch data.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">to_dict</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/flexit/train.html#TrainerMetrics.to_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Convert the metrics to a dictionary format.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">from_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">TrainerMetrics</span></span></span><a class="reference internal" href="_modules/flexit/train.html#TrainerMetrics.from_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Create a TrainerMetrics object from a dictionary.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">epochs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><code class="xref py py-class docutils literal notranslate"><span class="pre">list</span></code><span class="pre">[</span><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code><span class="pre">]</span></em></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">train_losses</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><code class="xref py py-class docutils literal notranslate"><span class="pre">list</span></code><span class="pre">[</span><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code><span class="pre">]</span></em></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">val_losses</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><code class="xref py py-class docutils literal notranslate"><span class="pre">list</span></code><span class="pre">[</span><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code><span class="pre">]</span></em></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">train_times</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><code class="xref py py-class docutils literal notranslate"><span class="pre">list</span></code><span class="pre">[</span><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code><span class="pre">]</span></em></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">learning_rates</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><code class="xref py py-class docutils literal notranslate"><span class="pre">list</span></code><span class="pre">[</span><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code><span class="pre">]</span></em></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_loss</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">val_loss</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epoch_time</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epoch</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/train.html#TrainerMetrics.update"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Update metrics with the latest epoch data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>train_loss</strong> (<em>float</em>) – The training loss for the epoch.</p></li>
<li><p><strong>val_loss</strong> (<em>float</em>) – The validation loss for the epoch.</p></li>
<li><p><strong>epoch_time</strong> (<em>float</em>) – The time taken for the epoch.</p></li>
<li><p><strong>lr</strong> (<em>float</em>) – The learning rate used for the epoch.</p></li>
<li><p><strong>epoch</strong> (<em>int</em>) – The current epoch number.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">to_dict</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/train.html#TrainerMetrics.to_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Convert the metrics to a dictionary format.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A dictionary representation of the training metrics.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">classmethod</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/train.html#TrainerMetrics.from_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Create a TrainerMetrics object from a dictionary.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>data</strong> (<em>dict</em>) – A dictionary containing the training metrics.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>An instance of TrainerMetrics populated with the provided data.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>TrainerMetrics</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">flexit.train.</span></span><span class="sig-name descname"><span class="pre">DummyOptimizer</span></span><a class="reference internal" href="_modules/flexit/train.html#DummyOptimizer"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code></p>
<p>Dummy optimizer for evaluation mode</p>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/train.html#DummyOptimizer.step"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Perform a single optimization step to update parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>closure</strong> (<em>Callable</em>) – A closure that reevaluates the model and
returns the loss. Optional for most optimizers.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">set_to_none</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/train.html#DummyOptimizer.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Reset the gradients of all optimized <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code> s.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>set_to_none</strong> (<em>bool</em>) – instead of setting to zero, set the grads to None.
This will in general have lower memory footprint, and can modestly improve performance.
However, it changes certain behaviors. For example:
1. When the user tries to access a gradient and perform manual ops on it,
a None attribute or a Tensor full of 0s will behave differently.
2. If the user requests <code class="docutils literal notranslate"><span class="pre">zero_grad(set_to_none=True)</span></code> followed by a backward pass, <code class="docutils literal notranslate"><span class="pre">.grad</span></code>s
are guaranteed to be None for params that did not receive a gradient.
3. <code class="docutils literal notranslate"><span class="pre">torch.optim</span></code> optimizers have a different behavior if the gradient is 0 or None
(in one case it does the step with a gradient of 0 and in the other it skips
the step altogether).</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">flexit.train.</span></span><span class="sig-name descname"><span class="pre">DummyScheduler</span></span><a class="reference internal" href="_modules/flexit/train.html#DummyScheduler"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Dummy scheduler for evaluation mode</p>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/train.html#DummyScheduler.step"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">flexit.train.</span></span><span class="sig-name descname"><span class="pre">Trainer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scheduler</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_fn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_dataloader</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">val_dataloader</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_accumulation_steps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fast_dev_run</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">callbacks</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/train.html#Trainer"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Lightweight trainer for transformer models with callback support.</p>
<p>This trainer class provides a flexible infrastructure for training transformer models
with support for various features like callbacks, checkpointing, and different model types.</p>
<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">device</span></span></dt>
<dd><p>Device to run training on (‘cuda’ or ‘cpu’)</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">model</span></span></dt>
<dd><p>The transformer model to train</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.nn.Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">optimizer</span></span></dt>
<dd><p>Optimizer for model training</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.optim.Optimizer</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">scheduler</span></span></dt>
<dd><p>Learning rate scheduler</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.optim.lr_scheduler._LRScheduler</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">loss_fn</span></span></dt>
<dd><p>Loss function for training</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Callable</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">train_dataloader</span></span></dt>
<dd><p>DataLoader for training data</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>DataLoader</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">val_dataloader</span></span></dt>
<dd><p>DataLoader for validation data</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>DataLoader</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">fast_dev_run</span></span></dt>
<dd><p>If True, runs only one batch for training and validation</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>bool</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">model_type</span></span></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Literal[‘encoder-decoder’, ‘encoder-only’, ‘decoder-only’]</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">Type</span> <span class="pre">of</span> <span class="pre">transformer</span> <span class="pre">model</span></span></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">grad_accumulation_steps</span></span></dt>
<dd><p>Number of steps to accumulate gradients</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">metrics</span></span></dt>
<dd><p>Object to track training metrics</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>TrainerMetrics</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">current_epoch</span></span></dt>
<dd><p>Current training epoch</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">train_state</span></span></dt>
<dd><p>Object tracking training state</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>TrainState</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">console</span></span></dt>
<dd><p>Rich console for pretty printing</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Console</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">callbacks</span></span></dt>
<dd><p>List of callbacks for training</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>list[Callback]</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">stop_training</span></span></dt>
<dd><p>Flag to stop training</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>bool</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TransformerModel</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">StepLR</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">optimizer</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">step_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">scheduler</span><span class="o">=</span><span class="n">scheduler</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">train_dataloader</span><span class="o">=</span><span class="n">train_loader</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">val_dataloader</span><span class="o">=</span><span class="n">val_loader</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span>
<span class="gp">... </span>        <span class="n">CheckpointCallback</span><span class="p">()</span>
<span class="gp">... </span>    <span class="p">],</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span> <span class="o">=</span> <span class="p">(</span>
<span class="gp">... </span>    <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
<span class="gp">... </span>        <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span>
<span class="gp">... </span>    <span class="p">)</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epochs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/train.html#Trainer.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Train the model for the specified number of epochs.
This method handles the complete training loop including validation if a validation
dataloader is provided. It manages callbacks, metrics tracking, and supports
early stopping.
:type epochs: <span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>
:param epochs: Number of training epochs to run.
:type epochs: int</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><dl class="simple">
<dt>Object containing training metrics including train/val losses,</dt><dd><p>learning rates, and epoch times.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>TrainerMetrics</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<ul class="simple">
<li><p>If <cite>fast_dev_run</cite> is <cite>True</cite>, only one batch will be used for training and validation</p></li>
<li><p>Training can be stopped early by setting stop_training to True via callbacks</p></li>
<li><p>Progress is logged to console using rich formatting</p></li>
<li><p>Callbacks are executed at the start of training, end of each epoch,and end of training</p></li>
</ul>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">save_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/train.html#Trainer.save_checkpoint"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Save the current training state to a checkpoint file.</p>
<dl class="simple">
<dt>The checkpoint contains:</dt><dd><ul class="simple">
<li><p>Current epoch number</p></li>
<li><p>Model state dictionary</p></li>
<li><p>Optimizer state dictionary</p></li>
<li><p>Learning rate scheduler state dictionary (if exists)</p></li>
<li><p>Training metrics</p></li>
<li><p>Model type</p></li>
<li><dl class="simple">
<dt>Training state including:</dt><dd><ul>
<li><p>Current step</p></li>
<li><p>Accumulation step</p></li>
<li><p>Number of processed samples</p></li>
<li><p>Number of processed tokens</p></li>
<li><p>Last completed epoch</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>path</strong> (<em>Path</em>) – Path where to save the checkpoint file.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span></p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">trainer</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">Path</span><span class="p">(</span>
<span class="gp">... </span>        <span class="s1">&#39;checkpoints/model.pt&#39;</span>
<span class="gp">... </span>    <span class="p">)</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">load_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">load_optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/train.html#Trainer.load_checkpoint"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Load training state from checkpoint.
This method restores model state, optimizer state, scheduler state, metrics, and training
state from a saved checkpoint file.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>path</strong> (<em>Union</em><em>[</em><em>Path</em><em>, </em><em>str</em><em>]</em>) – Path to the checkpoint file.</p></li>
<li><p><strong>load_optimizer</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to load optimizer state. Defaults to True.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>FileNotFoundError</strong> – If checkpoint file does not exist at specified path.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span></p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The checkpoint file is expected to contain some or all of the following:
- model_state_dict: State of the model
- optimizer_state_dict: State of the optimizer (if load_optimizer=True)
- scheduler_state_dict: State of the learning rate scheduler
- metrics: Training metrics history
- train_state: Training state including steps and counters
- epoch: Last completed epoch number</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">evaluate</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/train.html#Trainer.evaluate"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Evaluate the model on the validation set.</p>
<p>Runs the model in evaluation mode on the validation dataset and computes the validation loss
No gradients are computed during evaluation.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The validation loss value computed over the entire validation set.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">src</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">50</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">start_symbol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/train.html#Trainer.predict"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Generate predictions using model’s decoding strategy.
This method generates predictions based on the model type. For encoder-only models,
it performs direct classification. For encoder-decoder models, it uses greedy decoding
to generate sequences.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>src</strong> (<em>torch.Tensor</em><em>, </em><em>optional</em>) – Input tensor for prediction. Required for
encoder-only models.</p></li>
<li><p><strong>src_mask</strong> (<em>torch.Tensor</em><em>, </em><em>optional</em>) – Mask for source sequence. Required for
encoder-decoder models.</p></li>
<li><p><strong>max_len</strong> (<em>int</em>) – Maximum length for sequence generation. Defaults to 50.</p></li>
<li><p><strong>start_symbol</strong> (<em>int</em>) – Starting token for sequence generation. Defaults to 0.</p></li>
<li><p><strong>torch.Tensor</strong> – Predictions from the model. For encoder-only models, returns class
indices. For encoder-decoder models, returns generated sequences.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – If src is not provided for encoder-only models, or if either src or
    src_mask is not provided for encoder-decoder models.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="flexit-utils-module">
<h2>flexit.utils module<a class="headerlink" href="#flexit-utils-module" title="Link to this heading"></a></h2>
<p>Utility Functions</p>
<p>This module implements various utility functions used throughout the transformer library,
including layer cloning, mask creation, and other helper functions.</p>
<dl class="py function">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">flexit.utils.</span></span><span class="sig-name descname"><span class="pre">clone</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_clones</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/utils.html#clone"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Produce n_clones identical layers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> (<em>nn.Module</em>) – Module to clone.</p></li>
<li><p><strong>n_clones</strong> (<em>int</em>) – Number of clones.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>List of cloned modules.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>nn.ModuleList</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">flexit.utils.</span></span><span class="sig-name descname"><span class="pre">subsequent_mask</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cpu'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/flexit/utils.html#subsequent_mask"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Create a mask to hide future positions <cite>Causal mask</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>size</strong> (<em>int</em>) – Size of the mask.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Subsequent mask tensor.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-contents">
<h2>Module contents<a class="headerlink" href="#module-contents" title="Link to this heading"></a></h2>
<p>FlexiTransformers Module Components</p>
<p>This module provides the core components of the FlexiTransformers library,
organizing various transformer architecture elements including attention mechanisms,
positional encodings, and model structures.</p>
<p>Components:
- Attention: Multiple attention implementations (Absolute, ALiBi, Relative, Rotary)
- Callbacks: Training utilities for checkpointing and early stopping
- Configs: Configuration descriptors for model instantiation
- Core: Fundamental transformer building blocks (Encoder, Decoder)
- Layers: Basic neural network components (LayerNorm, FeedForward)
- Models: Complete transformer implementations with specialized variants
- Positional Encodings: Various embedding strategies for sequence positions
- Training: Utilities for efficient model training and evaluation</p>
<p>Each component is designed to be modular and composable, allowing for
flexible architecture design while maintaining interoperability.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="modules.html" class="btn btn-neutral float-left" title="flexit" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Ahmed Elshahawy.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>